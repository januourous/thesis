\documentclass[12pt,oneside]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%   Zusaetzliche Pakete  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{acronym}
\usepackage{enumerate}
\usepackage{a4wide}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{palatino}
\usepackage{blindtext}
\usepackage{multirow}
\usepackage[ruled,longend]{algorithm2e}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}


%folgende Zeile auskommentieren für englische Arbeiten
%\usepackage[ngerman]{babel}

\usepackage[bookmarks]{hyperref}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a-1b]{pdfx}
\usepackage[justification=centering]{caption}
%\usepackage[style=unsrt,natbib=true,backend=biber]{biblatex}
\usepackage{csquotes}
\usepackage{url}

\usepackage{minted}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{multicol}
\usepackage{mdframed}
\usepackage[most]{tcolorbox}
\usepackage[T1]{fontenc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Definition der Kopfzeile %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}
\setlength{\headheight}{16pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Definition des Deckblattes und der Titelseite  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\HSFTitle}[8]{

  \thispagestyle{empty}
\begin{center}
    \includegraphics[width=0.8\textwidth]{logo.eps} \\
    \vspace*{\stretch{1}}
    \end{center}

  %\vspace*{\stretch{1}}
  {\parindent0cm
  \rule{\linewidth}{.7ex}}
  \begin{center}
    \vspace*{\stretch{1}}
    \sffamily\bfseries\Huge
    #1\\
    \vspace*{\stretch{1}}
    \sffamily\bfseries\large
    #3
    \vspace*{\stretch{1}}
  \end{center}
  \rule{\linewidth}{.7ex}

  \vspace*{\stretch{2}}
  \begin{center}
    %\Large #2 am #5 der HAW Fulda \\
    \Large #5 \\
    \vspace*{\stretch{1}}

    \large Matriculation No:  #4 \\[1mm]
    \large Supervisor:  #7 \\[1mm]
    \large Co-Supervisor:  #8 \\[1mm]

    \vspace*{\stretch{1}}
    \large Submitted on #6
  \end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Beginn des Dokuments  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

  \HSFTitle
      {A study of reinforcement learning algorithms in simulated robotics scenarios }        % Titel der Arbeit
      {Masters Thesis} % Typ der Arbeit
      {Alejandro Pajares Chirre}          % Vor- und Nachname des Autors
      {1331534}
      {Masters Thesis submitted to the Faculty of AI at HS Fulda}  % Name des FBs
      {dd.mm.yyyy}        % Tag der Abgabe
      {Prof. Dr. Alexander Gepperth}     % Name des Erstgutachters
      {Prof. Dr. David James}    % Name des Zweitgutachters

  \clearpage

\lhead{}
\pagenumbering{Roman}
    \setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Kurzzusammenfassung   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
%
\markboth{Abstract}{Abstract}
\section*{Abstract}

The world of robotics has seen significant advancements in recent years, and this is largely due to the integration of machine learning techniques. Robots are now able to learn from their surroundings, make decisions, and carry out tasks with minimal human intervention. 
Machine learning has enabled robots to interact with humans and perform tasks that were previously considered impossible. In particular, reinforcement learning (RL) is a type of machine learning that models how humans learn from sensory input and motor responses in response to rewards. RL is based on the idea that an agent interacts with an environment by taking actions and receiving feedback in the form of rewards or punishments. Q-learning is a popular algorithm used in RL to learn the optimal policy, i.e., the best sequence of actions to maximize reward, for an agent. 
This thesis focuses on the application of reinforcement learning (RL) and Q-learning algorithms in controlling the motion of three joints of a six-degree-of-freedom (6-DOF) robotic arm in a simulated environment. To apply the Q-learning algorithms, the problem needs to be modeled as a Markov Decision Process, and during the learning process, the exploration and exploitation rate need to be balanced.
The robotic arm is modeled in Gazebo, and the control commands are sent using the Robot Operating System (ROS 2). The objective of the robotic arm is to touch the target. The RL algorithm learns to maximize the reward function, which is based on the current and previous distance between the target and one end of the robotic arm and the angles of the three joints being controlled. 
The experimental results demonstrate that RL and Q-learning algorithms can effectively control the motion of a robotic arm in a simulated environment. The robotic arm successfully learns to approach and touch the target.



% \par\noindent\rule{\textwidth}{0.4pt}

%The length of a thesis can vary depending on the subject. However, 50-55 pages for a master thesis is common, a little less for a bachelor thesis.
%Please pay careful attention to visual impression (sufficiently many and nicely made pictures/graphs, elegant formatting) as well as language (flawless English, elegant formulations), since these two points have are important for the grade. Furthermore, the introduction, discussion and conclusion chapters have a high impact on the grade, since people often read only those (which means they should be nice). 
\clearpage
\tableofcontents
\clearpage

\addcontentsline{toc}{section}{\listfigurename}
\listoffigures

\addcontentsline{toc}{section}{\listtablename}
\listoftables
\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Einstellungen  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\pagenumbering{arabic}
    \setcounter{page}{1}
\lhead{\nouppercase{\leftmark}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Hauptteil  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction} \label{sec:einleitung}

Designing, building, operating, and programming robots is the primary objective of the engineering and scientific discipline of robotics. A robot is a device created to carry out operations mechanically or somewhat autonomously, frequently imitating human actions or behavior \cite{1_o2019legal}. Numerous industries and applications, including agriculture, manufacturing, healthcare, transportation, entertainment, and the military, use robots. Robots are frequently utilized in industry to complete tasks like welding, creating art, integration, and packing that could be repetitious or dangerous for human workers. Robots are employed in agriculture to do duties including planting, harvesting, and crop monitoring \cite{2_roldan2018robots}. Robots are utilized in the transportation industry for logistics, warehouse management, and autonomous driving. Robots are employed in the entertainment industry for activities including animatronics as well as special effects. Robots are employed in the military for operations including bomb disposal, surveillance, and reconnaissance. Robots are evolving rapidly in terms of versatility, intelligence, and adaptability, as well as in terms of possible applications. Robotics is an emerging discipline that has the potential to significantly alter numerous aspects of our everyday lives and will probably become more crucial in determining our future \cite{3_bostrom2018ethics}.

Numerous companies have embraced the use of robotics to aid humans in tasks that are monotonous, physically demanding, or hazardous. Yet, acquiring a robot and hiring a robotic engineer to create a tailored solution for a particular task requires a significant investment of resources.
The duties of a robot engineer include setting up communication, designing control scripts, computing coordinate transformations, and creating error-handling programs. Typically, a technician takes on the task of operating the robot on a daily basis, or the robot operates on its own. However, if the task requirements or processes change, it is difficult to modify the existing robotic solution to suit a new configuration or application without the assistance of a robot engineer, despite the significant resources invested in acquiring and developing it.
Instead of relying on a robotic engineer to manually program a robot's operations for a new application, companies could employ deep reinforcement learning to train an intelligent agent to control the robot specifically for that application. This approach would enable the resources invested in robotics to be more adaptable and versatile, suitable for a broader range of applications and purposes.

This thesis is about reinforcement learning which is a type of machine learning that involves an agent learning from its interactions with an environment in order to maximize a reward signal over time \cite{suttonAndBarto}. It is a method of learning that involves trial and error, with the agent receiving feedback in the form of rewards or punishments for its actions. According to Kaelbling, Littman, and Moore \cite{kaelblingLittmanAndMoore}, reinforcement learning can be defined as "a problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment".
Many different concepts and methodologies can be used to break down reinforcement learning. This work focuses on Deep Q learning and to study it a simulated robotic arm is trained to touch a can.

% Generally, the introduction should be VERY detailed, with a focus on pedagogical value. 10-15 pages are expected here! Everything that contributes to clarity (pictures, diagrams, ...) is allowed or even expected. The introduction is meant for people who DO NOT have a computer science degree, or even any particular affinity to computers, so focus on the "big picture".  
%
\subsection{Context}
%What is the context of the presented work? If you work in a company, present the company first. Then the broad scientific or technological background in which the work is embedded should be presented and explained.
\subsubsection{Simulated Robotics} 
Simulated robotics is a rapidly developing field of study that strives to create intelligent systems that can communicate with virtual worlds in a manner that is comparable to how actual robots communicate with the real world \cite{4_choi2021use}. Compared to traditional robots, simulated robotics has many benefits, such as lower costs, more flexibility, and the ability to test and improve algorithms in a secure setting. Simulated robotic systems can be applied to a variety of tasks, from straightforward ones like object detection and manipulation to more difficult ones including autonomous navigation, group decision-making, and experience-based learning.  
The application of virtual environments for autonomous vehicle training and testing is an illustration of simulated robotics \cite{5_elmquist2021sensor}. To train and test the computer programs that operate autonomous vehicles, researchers can develop realistic simulations of numerous driving circumstances and scenarios, like traffic patterns, atmospheric conditions, and unforeseen incidents. With this strategy, researchers may evaluate the dependability and safety of autonomous vehicles in a safe setting before placing them on public roads. The application of virtual environments to train and test robots for search and rescue missions is another illustration of simulated robotics \cite{6_sampedro2019fully}. In order to train and test robotics that can aid in rescue operations, researchers can develop simulations of emergencies such as earthquakes or floods. With this method, researchers may test the efficacy and security of various robotic systems and algorithms in a range of circumstances without endangering human responders.
Systems for industrial automation can also be developed using simulated robotics. For instance, manufacturers can build and test robotic assembly lines, improve production workflows, and spot possible bottlenecks or safety risks using simulations. Before installing physical systems in the real world, this method enables manufacturers to optimize their procedures and increase productivity.
\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{gazebo_city}
\caption{Simulated urban scenario created in Gazebo \cite{yigit2020real}.}
\label{fig:gazebocity}
\end{figure}




\subsubsection{Industrial Automation}
Automation of industrial processes, such as manufacturing and construction, via the use of technological devices and control systems is known as industrial automation \cite{7_leitao2016industrial}. Industrial automation can be accomplished utilizing simulated robotic arms to carry out operations that would typically be carried out by human workers or actual robots. This entails creating the algorithms and control frameworks necessary for the virtual robotic arm to move and handle items precisely and effectively, as well as incorporating the virtual arm into a larger system that is capable of carrying out difficult tasks on its own.
Applications for simulated robotic arms in industrial automation include material handling, manufacturing work, quality control examinations, and machine maintaining. Companies can decrease the expenses of real robots and human labor while boosting production and efficiency by deploying virtual robotic arms \cite{8_javaid2021substantial}. Additionally, for increased flexibility and scalability, simulated robotic arms can be flexibly customized and adapted to various industrial environments. All things considered, the employment of simulated automated arms for automation in industry is a promising area of study and development, with tremendous potential for enhancing industrial procedures and developing the field of robotics.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{gazebo_city1}
\caption{Small Warehouse Gazebo simulation \cite{aws-robomaker-small-warehouse-world}.}
\label{fig:gazebocity1}
\end{figure}


\subsubsection{Robotic Arm}
Traditionally developed industrial robots have been confined to closed cells or limited-access warehouse areas \cite{9_lucchi2020robo}. They typically perform repetitive operations on standardized objects without human interaction. Programming these robots is usually a time-consuming process that requires specialized knowledge of the machine's software. However, current trends in robotics aim to make robots capable of operating in dynamic and open environments where they can work alongside humans. This presents new challenges that require equipping the robot with sensors to perceive its surroundings and interact with objects. However, integrating and utilizing sensor data for planning the robot's actions is not an easy task.
A robotic arm functions like a human arm and is a mechanical system that typically includes an end effector for manipulating and interacting with the environment \cite{10_ohta2018design}. Robotic arms have various applications in industrial and service fields, such as pick and place, exploration, manufacturing, laboratory research, and space exploration. The 6 degrees of freedom allow the arm to pivot in six different directions, similar to a human arm. In industrial robotic arms, the mechanical structure and control mechanism are major factors of concern.
These arms are commonly used in a variety of applications, such as manufacturing, assembly, material handling, and surgery. Robotic arms can be controlled by various means, including joystick or slider controls, keyboard-based interfaces, and programmed motions. They can be programmed to perform repetitive tasks with high precision and speed, which makes them ideal for use in industrial settings where consistency and efficiency are crucial. These arms can be equipped with various end-effectors, such as grippers, cameras, or welding tools, depending on the specific task that needs to be performed. They can also be designed to have multiple joints or degrees of freedom, which enables them to move in a wide range of directions and perform complex tasks. Advancements in robotics technology have led to the development of lightweight and portable robotic arms that can be easily integrated into various systems \cite{11_singh2013evolution}. These arms are becoming increasingly popular in areas such as healthcare and rehabilitation, where they can assist with tasks such as lifting and moving patients.
The primary focus of current research efforts is on training the robot’s arm to carry out various tasks autonomously using deep learning technologies. However, due to the massive amount of data required to teach a robot effectively, a data-driven approach is necessary. This can be challenging to achieve using a physical robotic arm. Therefore, developers have turned to robot simulation software \cite{12_oRobotics}, \cite{13_pybullet} to overcome the limitations of data-intensive AI approaches and to provide a stable environment \cite{14_banks1999introduction}. In a simulated environment, it is possible to control every aspect of the world, including impractical factors in reality. Moreover, there is no risk of damaging robots or human operators in simulations, and time control allows for faster data collection.

\subsubsection{Robot Operating System (ROS)}
The Robot Operating System (ROS) is a set of software libraries and tools that enables developers to build robotic applications \cite{15_quigley2009ros}. It provides a framework for writing and running code across multiple computers and devices, making it easier to create complex robotic systems. ROS was first developed in 2007 by Willow Garage, at robotics research lab \cite{16_cousins2014willow}. Since then, it has become widely adopted by the robotics community and is now supported by the Open Robotics organization. It offers a comprehensive platform for managing robotic systems. Originally designed to facilitate research in robotics, ROS is a unique framework. To grasp the fundamentals of the ROS framework, it is essential to comprehend the concept of message communication between nodes using topics.
One of the key features of ROS is its ability to handle communication between different components of a robotic system, such as sensors, actuators, and controllers \cite{17_emmi2014new}. This communication is done using a publish-subscribe messaging system, which allows components to share data and commands in real-time. ROS also provides a wide range of tools and libraries for tasks such as perception, navigation, and manipulation, which can be used to build complex robotic applications. These tools include algorithms for object recognition, path planning, and motion control, among others. Another advantage of ROS is its open-source nature, which means that developers can contribute to the development of the software and share their own code with the community. This has led to a large and active community of developers working on ROS, which has helped to drive its development and adoption.

\subsubsection{Simulations}
Simulations serve as an entry point for Digital Twins, which are highly accurate depictions of the physical world \cite{19_lu2020digital}. These Twins can aid in boosting manufacturing output and improving the flexibility of supply chains. To streamline the implementation of manufacturing processes during production line changes, digital twinning involves linking simulation software to an actual self-governing robotic system. A robotic arm digital twin solution is showcased in a recent study \cite{20_tavares2018flexible}, where the authors employed ROS \cite{21_ros} to achieve smooth functioning between the virtual and real worlds. Simulating software has its limitations as it cannot accurately represent the real world due to the imperfections in their physics engines. Additionally, simulations have the advantage of providing perfect data with no interference, which has supported the exploration of deep learning approaches in robotics research.
Simulations are a powerful tool for designing and testing robotic arms. They allow engineers to create virtual models of robotic arms and simulate their behavior in different scenarios, without the need for a physical prototype. In a robotic arm simulation, the arm's mechanical structure, control system, and sensors are modeled in a virtual environment. The simulation can then be used to test the arm's performance in various tasks, such as picking and placing objects, assembling parts, or performing complex movements. One of the key benefits of using simulations for robotic arm design is that they can help identify potential issues or inefficiencies before a physical prototype is built \cite{22_browning2001applying}. This can save time and resources, as well as improve the overall design of the arm. Simulations can also be used to optimize the control system of a robotic arm. By simulating the arm's behavior in different scenarios, engineers can identify the optimal control strategy for achieving a specific task or movement. 
The ROS framework includes a useful tool called RVIZ, which enables us to observe the robot's pose or estimation in a 3D environment \cite{23_kulkarni2021visual}. With the correct configuration of the URDF file, the robot model can be visualized in RVIZ.  Furthermore, simulations can help train and test algorithms for robotic arm control, such as RL methods or deep learning approaches. This can be done by running simulations with different environments and scenarios and using the resulting data to train and refine the algorithms.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{gazebo_robotic_arm}
\caption{Gazebo simulation of a Robotic Arm \cite{yigit2020trajectory}.}
\label{fig:gazeboroboticarm}
\end{figure}


\subsubsection{Reinforcement Learning}
Artificial neural networks (ANNs) are gaining importance in the field of robotics. In 2016, Levine et al.'s study \cite{24_ball2017comprehensive} provided encouraging outcomes, indicating a direction toward a more straightforward approach to constructing robot behaviors. The end-to-end approach described in the study is more scalable than traditional programming methods. To meet the demand for autonomous systems that cater to societal needs, it is necessary to replace conventional, unsophisticated autonomous systems with intelligent ones. Intelligent systems can take on various forms of intervention, such as aiding humans with image and video analysis, language translation, simplifying sentences, solving math problems, managing portfolios, undertaking monotonous tasks in the manufacturing industry, driving cars, flying helicopters, and more. 
The complexity of certain tasks makes it impractical to solve them by specifying a set of rules due to the vast number of rules required, and programming an agent's behavior in advance is also difficult. However, machine learning techniques can be used to develop learned agents capable of addressing these challenges. Supervised learning techniques require large amounts of labeled data for training, making them less practical for certain scenarios. Unsupervised learning, on the other hand, is not well-suited to situations that involve interaction with the environment.
Reinforcement learning (RL) \cite{24_ball2017comprehensive} presents new opportunities for addressing various challenges. In the past, RL techniques were only effective in domains where handcrafted features or low-dimensional input were utilized, and could only handle discrete state and action spaces. However, recent advancements in Deep Learning (DL) have yielded promising results in several fields such as speech, vision, wireless communication, natural language translation, and assistive devices. These advancements in DL-based techniques have made it possible to overcome the challenges faced by RL, and to process raw data for better performance.
The progress in deep learning techniques has paved the way for resolving the difficulties encountered in RL and processing raw data. The latest developments that merge RL with deep learning, such as those highlighted in \cite{26_bonsai}, \cite{27_mnih2016asynchronous} increase their suitability to domains where handcrafted features are not present, and the state or action space is either vast or continuous. Such advanced methods can address both perception and planning issues, whereas RL can only handle the planning problem and deep learning only the perception problem.
Rewarding or punishing agents for their actions, reinforcement learning (RL) is a kind of machine learning that enables agents to learn through interactions with an environment. RL deals with the development of decision-making algorithms. Its main focus is on creating a set of actions that an agent can perform in a specific environment. At each time step, the agent takes an action and receives feedback in the form of an observation and a reward. The ultimate goal of RL is to maximize the agent's total reward through a learning process that involves experimentation and feedback. 
In the learning process, an agent creates a policy. The agent begins in a particular state 's' of the environment, performs an action 'a', and transitions to another state 's'. The agent receives scalar feedback in the form of a reward 'r' after taking the action. This cycle is repeated many times during the learning process, as illustrated in Figure 1.
\par\noindent\rule{\textwidth}{0.4pt}
 The action space represents the possible actions that the agent can take in a given state. In the past, the tabular approach was used to store state and action values. However, in environments with a large number of states or actions, the approximation approach has replaced the tabular method. Neural networks are commonly used as approximation methods, as shown in Figures 2a and b. There are two types of action spaces, as illustrated.
 \par\noindent\rule{\textwidth}{0.4pt}
Deep Reinforcement Learning (DRL) is a field of study in which neural networks are utilized as function approximators to improve the performance of RL. Recently, several DRL techniques have achieved remarkable success in learning complex behavior skills and solving challenging control tasks in high-dimensional state-space \cite{28_wu2017scalable} environments. However, many benchmarked environments such as Atari \cite{29_mnih2013playing} and Mujoco \cite{30_todorov2012mujoco} lack complexity or realism, which is often present in robotics. Additionally, these benchmarked environments \cite{31_nogueira2017autonomously} do not use commonly used tools in the field such as ROS.
The research conducted in the previous work requires a considerable amount of effort for each specific robot, and therefore, the scalability of previous methods for modular robots is questionable. Consequently, trial and error learning process is often needed to apply the previous research to real-world robots. Training robotic arms to carry out certain actions, including touching an object in a virtual environment, is an intriguing use of RL. The robotic arm in this scenario must navigate a complicated state space while selecting actions that would maximize its reward while avoiding collisions with the environment. 

\subsubsection{Deep Reinforcement Learning}
Deep Reinforcement Learning (DRL) leverages deep learning architectures as function approximators to tackle high-dimensional data and address the challenge of approximating in the presence of large state and action spaces \cite{32_boute2022deep}. Unlike traditional methods such as decision trees or SVMs, DRL employs neural networks to map states to actions, enabling it to handle high-dimensional data types such as images, videos, and time-series data. The approach also utilizes deep learning techniques such as convolutional or recurrent neural networks to address the limitations of traditional artificial neural networks which are unable to handle such data and often ignore input data topology. Prior research has focused on addressing various challenges in applying DRL to different fields, particularly control problems and games like Atari. Some of the key tasks involved in DRL include:
\begin{itemize}
\item  \textbf{Exploration Exploitation: }The exploration refers to trying a new action, whereas exploitation makes use of learned knowledge to decide the action.
\item \textbf{Generalization:} Generalization, on the other hand, refers to the capability of the agent to adapt to new environments, which can range from one task to another or from simulation to a real-life situation.
\item \textbf{Finding Policy:} Finding a valuable policy involves identifying important states and actions that can help in learning an optimal policy for decision-making.
\item \textbf{Finding a catastrophe:} Discovering a catastrophic event is crucial, as such events may cause significant harm. These events may include physical harm, offensive tweets, false stories, and so on. Avoiding such occurrences can help to improve the policy.
\item \textbf{Handling Overestimation:} Overestimation occurs when inaccurate computation of action value takes place, often due to the use of the max operation in Q learning. It is important to handle overestimation to ensure that accurate values are computed.
\item \textbf{Reducing Sample Size:} Deep reinforcement learning (DRL) requires a large number of samples for effective training, which may not always be feasible in real-world scenarios. This poses a challenge for DRL applications that deal with limited data availability.
\item \textbf{Detection and prevention of overfitting:} overfitting is a common issue in DRL, especially when using high-capacity deep learning models. Overfitting occurs when the agent is too sensitive to small perturbations in the environment.
\item \textbf{Robust Learning:} In recent years, there has been a growing interest in incorporating robustness into the DRL system using techniques such as deep learning. Researchers have proposed various adversarial attacks and defense mechanisms to address this challenge. Therefore, enhancing the robustness of DRL systems has become an active research topic in the community.
\end{itemize}
 
The field of Reinforcement Learning (RL) has seen significant contributions from various researchers who have proposed different network architectures and action selection criteria. Some methods include trial and error without human intervention, learning via demonstrations, learning via criticism, and learning with adversaries to tackle complex problems. Despite these advancements, the challenge of discovering an optimal policy that is both robust and able to meet multiple goals remains a topic of active research and an open area of investigation.

\subsubsection{Double Q-Learning}
The Q-learning algorithm, a well-liked RL method, has a version called double Q-learning \cite{33_ogunniyi2014energy}. The fundamental tenet of Q-learning is to acquire knowledge of a function $Q(s,a)$, which denotes the anticipated reward for performing action in a given state s. The agent employs this feature to determine the optimal course of action in each state. However, Q-learning may be hampered by an overestimation of the Q-values, which could result in ineffective policies. This problem is addressed by double Q-learning, which estimates the greatest expected reward by switching between two different Q-functions.
The robot's arm would operate as the agent in a Double Q-learning simulation \cite{34_weng2020mean}, interacting with its surroundings and receiving rewards or punishments as a result of its actions. The orientation and position of the robotic arm, along with the location and characteristics of the object it is attempting to touch, would all be included in the state space. The robotic arm's range of potential motions for approaching the object would make up the action space. The goal of the reward function is to persuade the robotic arm to touch the target without causing a collision or any other unfavorable outcomes.
The Double Q-learning method is used by the robotic arm to investigate its environment during training and modify its Q-functions in response to incentives. By estimating the expected reward for every action using the two Q-functions, the algorithm would select the action featuring the largest expected reward. The robotic arm would develop a policy that enables it to consistently touch the target by continuing this process over a large number of attempts. In this thesis, we explore the use of Double Q-Learning in a simulated robotic arm to learn effective control policies for the arm.
%The simulated robotic arm used in this study consists of three joints, each of which can rotate through an angle of 360 degrees
%\cite{35_megalingam2013kinect}. The objective of the arm is to reach a target position specified by the RL algorithm. The state of the arm is represented by the joint angles, and the action is the change in joint angles to reach the target position. The reward function is defined as the negative Euclidean distance between the current position of the arm and the target position. The implementation of Double Q-Learning involves maintaining two Q-functions, Q1 and Q2, which are updated alternately. In each update, one of the Q-functions is used to select the best action, while the other Q-function is used to evaluate the selected action. This approach reduces the overestimation of action values, leading to more accurate estimation of the optimal policy.
%When compared, the performance of Double Q-Learning with that of Q-Learning and the Double Q-Learning outperformed Q-Learning in terms of convergence speed and final performance. The Double Q-Learning algorithm was able to learn an effective control policy for the robotic arm, enabling it to reach the target position with high accuracy and minimal error. To further evaluate the effectiveness of Double Q-Learning, we conducted experiments to test its robustness to changes in the target position and the reward function. We found that Double Q-Learning was able to adapt to changes in the target position and reward function, demonstrating its ability to learn a robust control policy for the robotic arm.


\subsection{Problem statement}

Many studies have demonstrated that utilizing reinforcement learning (RL) presented a viable solution for addressing the limitations of conventional methods in tackling intricate robotics tasks. Numerous AI experts have created various frameworks and toolkits to examine and assess their algorithms' effectiveness in solving challenging problems \cite{36_wu2020ethical}. Although the outcomes were remarkable, these applications were generally limited to simulated environments and seldom deployed in real-world scenarios. Numerous researchers are presently focused on a highly promising mission of bridging the gap between simulation and reality. However, proficiency in various domains is crucial in the intricate field of RL, which might be an obstacle to entry for roboticists.
For Problems involving an agent interacting with the environment to maximize a reward signal, RL, a potent branch of machine learning (ML), is used. Gaming, robotics, and finance are just a few of the industries where RL has been successfully used. RL has been applied to robotics to tackle a variety of problems, including grasping, manipulating, and navigating. One of these tasks involves teaching a virtual robotic arm to touch an object using reinforcement learning.
The objective of this proposed study is to create a simulated robotic arm that can learn the Double Q Learning method for RL in order to learn how to touch an object. The object should not be knocked over or sustain any damage when the robotic arm moves its end-effector opposite direction and touches it. 
The following are the study's primary obstacles:
\begin{itemize}
\item Designing the incentive function: It is difficult to create a reward mechanism that encourages the robotic arm to interact with the object without damaging it or knocking it over.
\item Tackling high-dimensional events and action spaces: It is difficult to apply RL methods to the robotic arm's highly dimensional state space and continuous action space.
\item Safeguarding the surroundings and the robotic arm is essential while instructing the agent.
\end{itemize}

We will utilize the Double Q Learning technique to address this issue since it uses two Q-value functions to address the overestimation problem with Q Learning. The best course of action is chosen using one Q-value function, and the chosen course of action is assessed using the other Q-value function. The environment and robotic arm will be simulated using a physical simulator. The robotic arm will use the Double Q Learning method to choose an action after receiving state information from the simulator. In order to give the agent knowledge about the next state and a reward signal, a simulator will model the dynamics of the robotic arm and the object.

%What is the problem that is being addressed or solved? Why is it important or beneficial to find a solution to this problem? 
%
\subsection{Goals}\label{sec:ziele}
The following are the main goals of this study:
\begin{itemize}
\item Use RL to instruct the robotic arm to approach and grasp an object.
\item To train a robot's arm, use the Double Q-learning algorithm.
\item Establish a reward mechanism to motivate the robotic arm to effectively touch the target.
\item Create a simulated space where the robotic arm is capable of touching things.
\item Define the robotic arm's state space in the simulation environment.
\item Define the robotic arm's reaction space in the simulation environment.
\item Train the robot's arm on the best ways to touch an object so it can learn them.
\item To get the greatest performance, fine-tune the Double Q-learning algorithm's hyperparameters.
\item By counting the number of times, the robotic arm successfully touches an object, you may evaluate how well it is working.
\item Continually enhance the training procedure by modifying the algorithm or its reward function as necessary.

\end{itemize}

Here, give a list of bullet points of quantifiable goals of the presented work. These achievement of these goals will be shown in the experiments section. The list should have 3-4 entries. No blabla here, hard goals!
%
\subsection{Related Work}
Deep Q-learning has been successfully applied in various robotic control tasks, including manipulators and robotic arms. In this section, we summarize the results and contributions of six relevant papers on deep Q-learning for robotic arm control.

In \cite{LevinePastorKrizhevsky}, the authors proposed a method for robotic grasping using deep Q-networks (DQNs). The results showed that the proposed method outperforms other traditional grasping methods in terms of grasping success rate and efficiency.

In \cite{GuShixiangHolly}, the authors presented a framework for real-time control of a robotic arm using deep reinforcement learning. They used a DQN-based algorithm to learn the control policy for the robotic arm, and the results demonstrated that the proposed method can achieve accurate and robust control.


\par\noindent\rule{\textwidth}{0.4pt}


List works that have a similar goals, plus a short explanation (3-4 sentences at most) as to how they differ from your work. Do NOT make comparisons here (better than my work or similar), that happens in the discussion section.

Admissible related work is (in descending order of acceptability):
\begin{itemize}
\item Peer-reviewed scientific publications, ideally with a DOI. Use Google scholar for searching (GS can export BibTeX entries that you can copy into the .bib file of this project). 

\item White papers and publicly available documents without review, cite with title, URL and date of access. In addition, you need to submit the PDFs in electronic form.

\item Web pages, especially for software projects (e.g., TensorFlow, nginx, react, Django). Cite via URL and date of access. A github/gitlab/etc link is acceptable as well. Nothing needs to be submitted electronically, but only use such sources of there is no other way. 
\end{itemize}

Literature is cited like this: as shown in {clemen1989combining}, blablaba. Or: \cite{clemen1989combining} has a similar scope in the domain of perverted numerical integration, however without considering the aspect of cupidity. Or: In \cite{clemen1989combining}, a study of perverted diagonal matrix perversions is presented. You need not include page numbers.

See also  Kap.~\ref{sec:zitate}, \ref{sec:webquellen}. 

\subsection{Contribution}
Here, you present a bullet ist of your personal contributions to the topic of the thesis. For example: 
\begin{itemize}
    \item Implementation of a bash script that did not work and was hard to read
    \item Comparison of different implementations for matrix perversion
    \item Implementation of a web service that provides jokes about professors via a ReST API.
\end{itemize}

\section{Foundations}\label{sec:grundlagen}
%The targeted group are computer scientists with at least a Bachelor's degree. Here, you explain aspects that go beyond what this group would not usually know.
\subsection{Python}
Python is a popular language for implementing deep learning algorithms because of its simplicity, adaptability, and abundance of tools and frameworks. It has grown in popularity as a programming language for machine learning, deep learning, and robotics due to its simplicity of use, wide library support, and adaptability. TensorFlow, PyTorch, Keras, OpenCV, NumPy, SciPy, ROS, and Gazebo are just a few of the libraries and frameworks available in Python for machine learning, deep learning, and robotics. These libraries and frameworks provide developers with a variety of tools and methods for developing and deploying machine learning, deep learning, and robotic applications. Python's readability and ease of use make it an appealing choice for machine learning and robotics developers. Its syntax is straightforward and clear, making it easier to develop and comprehend code. Furthermore, the dynamic nature of Python allows for quick prototyping and experimentation, which is vital for building and testing machine learning and robotics algorithms. Python's extensive library and frameworks, paired with its simplicity of use and adaptability, makes it an excellent choice for machine learning, deep learning, and robotics applications. Here are some of the most important Python modules and frameworks for these fields:
\begin{enumerate}
\item TensorFlow: Google's popular deep learning package that supports both static and dynamic computation graphs. TensorFlow is utilized in a variety of applications such as computer vision, natural language processing, and robotics.
\item PyTorch: A famous deep learning library created by Facebook that is noted for its simplicity and adaptability. PyTorch is utilized in a variety of applications such as computer vision, natural language processing, and robotics.
\item Keras: Keras is a high-level deep learning API that may be used in conjunction with TensorFlow, Theano, or CNTK. Keras is well-known for its simplicity and is frequently used for quick prototyping and experimentation.
\item OpenCV: OpenCV is a free and open-source computer vision toolkit that includes a variety of image processing and computer vision methods. OpenCV is frequently used in robotics applications like object identification and tracking.
\item NumPy: A Python library for numerical computing that supports massive, multi-dimensional arrays and matrices. NumPy is commonly used as a basis for many other scientific computing libraries, as well as in machine learning and robotics applications.
\item SciPy: A Python library for scientific computing that supports optimization, signal processing, and other scientific computing activities. For tasks such as optimization and control, SciPy is frequently utilized in machine learning and robotics applications.
\item Python: Python has a variety of robotics libraries, including ROS (Robot Operating System) and Gazebo, which are frequently used for designing and modeling robotic applications.
\end{enumerate}

Python's popularity in robotics is growing due to its simplicity of use, adaptability, and wide library support. Here are some of the reasons why Python is useful in robotics:
\begin{enumerate}
\item Python features a basic and easy-to-learn syntax, making it accessible to both beginners and professionals. Python's readability and compact syntax make it simple to create and comprehend code, which is required while designing and testing robotics algorithms.
\item Python is a versatile programming language that may be used for a variety of purposes, including robots. Python is a versatile choice for robotics developers since it can be used for both high-level and low-level programming.
\item Extensive library support: Python has a number of libraries and frameworks intended expressly for robotics, such as ROS, Gazebo, and PyBullet. These libraries and frameworks provide developers a variety of tools and algorithms for developing and delivering robotics applications.
\item Python's dynamic nature enables quick prototyping and experimentation, which is vital for designing and testing robotics algorithms. Python's interactive shell also makes it simple to test code and try out new ideas.
\item Integration with other languages: Python can readily integrate with other programming languages used in robotics, such as C++ and MATLAB. This enables robotics engineers, regardless of programming language, to employ the finest tools for the job.
\end{enumerate}

\subsubsection{Python Deque Data Structure}{\label{python:deque}}
A \texttt{deque} in Python is a double-ended queue data structure that allows efficient adding and removing of elements from both ends. It is provided as a built-in class in the \texttt{collections} module \cite{python-docs}.

The following are the main methods available for a \texttt{deque}:

\begin{itemize}
  \item \texttt{append(x)}: Adds an element \texttt{x} to the right end of the deque.
  \item \texttt{appendleft(x)}: Adds an element \texttt{x} to the left end of the deque.
  \item \texttt{pop()}: Removes and returns the rightmost element from the deque.
  \item \texttt{popleft()}: Removes and returns the leftmost element from the deque.
  \item \texttt{rotate(n)}: Rotates the deque \texttt{n} steps to the right (if \texttt{n} is positive) or to the left (if \texttt{n} is negative).
  \item \texttt{count(x)}: Counts the number of occurrences of an element \texttt{x} in the deque.
  \item \texttt{extend(iterable)}: Extends the deque by appending all elements from the iterable to the right end.
  \item \texttt{extendleft(iterable)}: Extends the deque by appending all elements from the iterable to the left end (in reverse order).
\end{itemize}

One of the main advantages of using a \texttt{deque} over a list for implementing a queue or a stack is that both append and pop operations have O(1) time complexity. This means that adding or removing elements from either end of the deque is very efficient, even for very large data sets \cite{python-docs}.


\subsection{Pytorch}
PyTorch is a Python-based open-source machine learning library based on the Torch library, which was created by Facebook's AI Research (FAIR) team \cite{37_wang2019various}. It is generally employed for creating deep learning models for applications like speech recognition, computer vision, and natural language processing. PyTorch uses a dynamic computational network to construct and adapt models, which gives it a competitive advantage over competing deep learning frameworks. It also provides a variety of tools and utilities for developing, training, and assessing deep learning models, including as data loaders, optimizers, and loss functions. One of PyTorch's primary advantages is its ability to easily integrate with Python, making it simple to use for developers and researchers who are already familiar with Python. Furthermore, PyTorch provides a user-friendly interface for developing and training deep learning models, reducing the time and effort necessary to design and deploy models.
PyTorch provides a versatile and fast framework for creating and training deep neural networks, making it an excellent candidate for DDQN implementation. Typically, the procedure entails establishing the neural network architecture, configuring the environment, and performing training loops to update the network weights depending on the agent's actions and rewards. PyTorch also has several important DDQN capabilities, such as the ability to build various optimization algorithms and loss functions, which can assist to increase the efficiency and efficacy of the learning process. Furthermore, PyTorch's automated differentiation capability can make it easier to construct and debug complicated DDQN algorithms by automatically calculating gradients during the training phase.

\subsection{Gazebo}
Gazebo is a free and open-source 3D simulation platform for robotics and automation \cite{38_abbyasov2020automatic}. It enables users to design and simulate complex systems like robots, sensors, and surroundings, and it is used for a variety of applications such as robot development, testing, and validation. Gazebo has a realistic physics engine that properly replicates object behavior and interactions with the environment, allowing developers to test and debug their algorithms in a safe and controlled setting. It also includes a variety of sensors and actuators for simulating various sorts of robotics and automation systems. Gazebo is built on top of the ODE (Open Dynamics Engine) physics engine and generates realistic images with the Ogre 3D rendering engine. It is developed in C++ and works with a variety of programming languages such as Python, Java, and MATLAB. 
Gazebo is frequently used in combination with other robotics libraries and frameworks, such as ROS (Robot Operating System), which offers a comprehensive collection of tools and utilities for developing and testing robotic systems. ROS contains a Gazebo integration package that enables smooth integration of the two platforms, making it simple to model and test robotic systems with Gazebo. Gazebo is largely used in robotics and automation applications for simulation. It offers a realistic simulation environment for testing and evaluating algorithms and systems before deploying them in the real world. Users may use Gazebo to design and simulate complex systems such as robots, sensors, and surroundings, as well as test their algorithms in a safe and controlled environment. The simulation environment contains a physics engine that realistically models object behavior and interactions with the environment, allowing developers to test and debug their algorithms and systems under a variety of scenarios. Gazebo offers a wide range of sensors and actuators, such as cameras, lidar, and GPS, that may be used to mimic many sorts of robots and automation systems \cite{39_echeverria2011modular}. It also allows plugins, which let users customize and enhance the simulation environment's capabilities to match their individual needs. Gazebo is often used in robotics research and development, as well as in robotics and automation system teaching and training. It may be used in conjunction with other robotics libraries and frameworks, such as ROS (Robot Operating System), to offer a full simulation and development environment for robotic systems.
Gazebo is commonly used for simulating robotic arms in robotics research and development. Gazebo offers a diverse set of sensors and actuators for simulating many sorts of robotic arms. Joint controllers, for example, can be used to regulate the position and velocity of the joints in the arm, and force/torque sensors can be used to mimic contact with objects and surfaces. Furthermore, Gazebo has a plugin system that allows developers to customize and enhance the simulation environment to match their individual requirements. Developers can write plugins, for example, to imitate certain sensors or actuators or to provide new control techniques for the robotic arm.

\subsection{ROS 2}
A collection of ROS software packages available for download is referred to as a ROS distribution, backed by the non-profit organization, Open-Source Robotics Foundations (OSRF) \cite{18_ebeid2018survey}. The ROS organization periodically updates these packages and assigns distinct titles to each distribution.
ROS2 (Robot Operating System 2) \cite{40_phueakthong2021development} is the second edition of the popular open-source robotics middleware framework, ROS, launched in 2017 to address some of the limits and issues of the original ROS framework. ROS2's primary features and benefits include the following:
\begin{enumerate}
\item Enhanced real-time performance: In comparison to the original ROS middleware, ROS2 incorporates a new middleware layer called the Data Distribution Service (DDS), which delivers enhanced real-time speed and stability.
\item Improved support for multi-robot systems: ROS2 introduces the ROS2 Multi-Robot System (MRS) architecture, which improves communication and coordination between numerous robots.
\item ROS2 features support for Transport Layer Security (TLS) and X.509 certificates, which enables increased security and authentication for node-to-node connections.
\item Better non-Unix platform compatibility: ROS2 has enhanced support for non-Unix systems such as Windows and macOS.
\item ROS2 features better development tools and documentation, making it easier for developers to get started with the framework.
\end{enumerate}

Conclusively, ROS2 is intended to provide a more robust, dependable, and adaptable platform for developing and deploying robotic systems. It is interoperable with a broad range of robotic hardware and software platforms, and it has a big and active development and user community.
In robotics research and development, ROS2 is often used to operate robotic arms. Developers may use ROS2 to design a software stack that contains the control algorithms, sensors, and communication interfaces needed to control the robotic arm. ROS2 is a flexible and modular design that enables developers to construct software modules known as nodes that connect with one another via a publish-subscribe messaging model. For example, one node may be in charge of reading sensor data from the robotic arm, while another node may be in charge of operating the motors of the arm depending on the sensor data. A large selection of software libraries and tools referred to as packages, are also offered by ROS2 and may be used to create and test robotic arm control software. For instance, the \textit{ros\_control} package offers a variety of hardware interfaces and controllers for interacting with robotic arm hardware, while the \textit{MoveIt} package offers a set of motion planning and control algorithms for robotic arms. Developers often begin by specifying the hardware interface for the arm, which comprises the joints, motors, and sensors, in order to control a robotic arm using ROS2. Then, using ROS2 nodes and packages, they create the control algorithms and sensor interfaces needed to control the arm. Finally, before implementing their robotic arm control software on the MoveItactual robot, engineers may test and evaluate it using simulation tools like Gazebo. This enables the software to be developed and iterated upon quickly without endangering the physical robot or its surroundings.

\subsubsection{ROS2 Nodes}\label{ros2-nodes}
ROS2 nodes are the fundamental building blocks of a ROS2 system. A node is a process that performs computation and communicates with other nodes in the ROS2 system. In ROS2, nodes are implemented using the rclpy.node.Node class in Python. This class provides a way to create a node and interact with the ROS2 middleware. It provides methods to create publishers, subscribers, services, clients, timers, and parameters. By inheriting from this class, a Python class can become a ROS2 node and use these methods to interact with the ROS2 system.
To create a ROS2 node, one needs to create a ROS2 package and a Python file inside it. The Python file should inherit from the $rclpy.node.Node$ class and override its $\_\_init\_\_()$ method to create publishers, subscribers, services, clients, timers, and parameters. The $main()$ function initializes the ROS2 system, creates an instance of the Python class, and spins the node to process callbacks. Finally, the $rclpy.shutdown()$ function is called to shut down the ROS2 system.\cite{ros2docs}
ROS2 nodes can be created using object-oriented programming (OOP) techniques. OOP is the recommended way to write a node in ROS2, and it works pretty well. OOP allows for better code organization, encapsulation, and reusability. It also makes it easier to write testable code.
In conclusion, ROS2 nodes are the fundamental building blocks of a ROS2 system. They are implemented using the rclpy.node.Node class in Python. By inheriting from this class, a Python class can become a ROS2 node and use its methods to interact with the ROS2 system. OOP is the recommended way to write a node in ROS2, and it allows for better code organization, encapsulation, and reusability.

\subsubsection{ROS2 Services}\label{ros2-services}
ROS2 services provide a way for nodes to communicate with each other by requesting and providing specific functionalities or operations. This is done through a client-server communication model where the node requesting the service acts as the client and the node providing the service acts as the server.

One advantage of ROS2 services is their ability to handle both synchronous and asynchronous communication. Synchronous communication blocks the client node until a response is received from the server node. Asynchronous communication allows the client node to continue with other operations while waiting for a response from the server node.

ROS2 services also provide a way to handle errors that may occur during service communication. In the case of a failure, the server node can send an error message to the client node, indicating the reason for the failure. This makes it easier for developers to debug their code and troubleshoot issues that may arise.

ROS2 services can easily scale. Multiple nodes can request the same service from a single server node, which can handle all requests concurrently. This can improve the overall performance of the system and reduce latency.

However, ROS2 services also have some limitations. For example, they do not support streaming data or continuous communication. They are intended for point-to-point communication, where a client node requests a specific operation from a server node and receives a single response.

In summary, ROS2 services provide a flexible and reliable way for nodes to communicate with each other by requesting and providing specific functionalities or operations. They are suitable for point-to-point communication and can handle both synchronous and asynchronous communication, making them a valuable tool for ROS2 developers.

\subsubsection{ROS2 Topics}\label{ros2-topics}
ROS2 topics provide a way for nodes to communicate with each other by exchanging messages on a specific topic. This is done through a publish-subscribe communication model where the node publishing the message acts as the publisher and the node receiving the message acts as the subscriber.

ROS2 topics can handle asynchronous communication. This means that the publisher node does not have to wait for the subscriber node to receive the message before continuing with other operations. This can improve the overall performance of the system and reduce latency.

ROS2 topics also provide a way to handle errors that may occur during message communication. In the case of a failure, the subscriber node can request that the publisher node resend the message or ignore the message and continue receiving subsequent messages. This makes it easier for developers to debug their code and troubleshoot issues that may arise.

Another advantage of ROS2 topics is their flexibility. Multiple nodes can subscribe to the same topic, and publishers can send messages to multiple subscribers simultaneously. This can improve the overall efficiency of the system and reduce the amount of code required to implement certain functionalities.

One limitation of ROS2 topics is that they do not provide any mechanism for ensuring that messages are received in a particular order, and they do not guarantee message delivery. Additionally, topics are not suitable for point-to-point communication, as any node subscribed to a topic will receive all messages published on that topic.

Finally, ROS2 topics provide a flexible and reliable way for nodes to communicate with each other by exchanging messages on a specific topic. They are suitable for asynchronous communication, and can handle multiple subscribers and publishers, making them a valuable tool for ROS2 developers. However, they have some limitations and may not be suitable for all communication scenarios.

\subsubsection{ROS2 Actions}\label{ros2-actions}
ROS2 actions provide a way for nodes to communicate with each other by executing a specific goal and receiving a result. This is done through an action client-server communication model where the node requesting the action acts as the client and the node executing the action acts as the server.

One advantage of ROS2 actions is their ability to handle long-running operations. Unlike ROS2 services, which are intended for point-to-point communication, ROS2 actions can handle operations that may take a significant amount of time to complete. This makes them ideal for tasks such as robot arm motion planning, where the task may take several seconds or even minutes to complete.

ROS2 actions also provide a way to handle feedback during the execution of an action. The server node can send periodic updates to the client node, indicating the progress of the operation. This makes it easier for developers to monitor the execution of an action and respond to any issues that may arise.

Another advantage of ROS2 actions is their ability to handle cancel requests. If the client node needs to abort the execution of an action, it can send a cancel request to the server node. The server node can then gracefully stop the execution of the action and return a result indicating that the action was canceled.

However, ROS2 actions also have some limitations. For example, they are more complex than ROS2 topics and services and require more code to implement. Additionally, they are not suitable for operations that do not have a clear goal and result, such as continuous data streams.

In the end, ROS2 actions provide a powerful and flexible way for nodes to communicate with each other by executing long-running operations and providing feedback and cancellation capabilities. They are suitable for tasks that require a clear goal and result, such as robot arm motion planning, and can handle operations that may take several seconds or minutes to complete. However, they are more complex to implement than ROS2 topics and services and may not be suitable for all communication scenarios.

\subsubsection{The tf\_ros.TransformListener}\label{ros2-transform-listener}
The tf2\_ros.TransformListener is a ROS2 utility class that allows a node to receive the transform between two frames from the tf2 system. This class provides a simple way to listen for and access transforms between frames in a ROS2 system.

To use the tf2\_ros.TransformListener, a node must first initialize a tf2\_ros.Buffer object. This buffer object is used to store and manage the transforms that the node receives from the tf2 system. Once the buffer is initialized, the node can then create an instance of the tf2\_ros.TransformListener class, passing in a reference to the buffer object.

Here is an example of how to use the tf2\_ros.TransformListener to get the transform between two frames:
\begin{listing}[H]
\caption{Example of how to use TransformListener to obtain the transform between two frames.}
\label{code:transformliseter}
\begin{minted}[fontsize=\scriptsize]{python}
import rclpy
import tf2_ros

def main(args=None):
    rclpy.init(args=args)

    # Initialize the buffer and listener
    buffer = tf2_ros.Buffer()
    listener = tf2_ros.TransformListener(buffer)

    # Wait for the transform to become available
    try:
        buffer.lookup_transform('map', 'robot', rclpy.time.Time())
    except tf2_ros.TransformException as ex:
        print(ex)

    # Get the transform between map and robot
    transform = buffer.lookup_transform('map', 'robot', rclpy.time.Time())

    # Print the transform
    print(transform)

    rclpy.shutdown()

if __name__ == '__main__':
    main()

\end{minted}
\end{listing} 

In this example, the node initializes a tf2\_ros.Buffer object and a tf2\_ros.TransformListener object. The node then waits for the transform between the map and robot frames to become available. Once the transform is available, the node retrieves it from the buffer and prints it to the console.

The tf2\_ros.TransformListener class is a useful tool for nodes that need to access transforms between frames in a ROS2 system. It provides a simple interface for listening for and accessing transforms, allowing nodes to easily interact with the tf2 system.

\subsubsection{The /joint\_trajectory\_controller/follow\_joint\_trajectory action server}\label{ros2-joint-trajectory-controller}
The $/joint\_trajectory\_controller/follow\_joint\_trajectory$ is an action server in ROS (Robot Operating System) that allows a client to send a joint trajectory command to a joint trajectory controller. This action server is part of the ros\_controllers package and is typically used to control the motion of a robot arm or any other mechanism with multiple joints.

The joint trajectory command is specified in a FollowJointTrajectory.Goal message, which contains a JointTrajectory message that specifies the desired joint positions, velocities, accelerations, and time stamps. The joint trajectory controller then uses this command to interpolate a trajectory and generate the corresponding joint motion for the robot.

The $/joint\_trajectory\_controller/follow\_joint\_trajectory$ action server follows the standard ROS action interface, which consists of a goal, feedback, and result. When a client sends a joint trajectory goal to the action server, it sends a FollowJointTrajectory.Goal message as the action goal. The action server then sends a FollowJointTrajectory.Feedback message to the client periodically during the trajectory execution, which can be used to provide real-time feedback on the robot's progress. Finally, when the trajectory execution is complete, the action server sends a FollowJointTrajectory.Result message to the client.

For example, assuming that we have a robotic arm with a joint trajectory controller set up in a ROS 2 system, the following steps demonstrate how to send a trajectory command to the controller using the $/joint\_trajectory\_controller/follow\_joint\_trajectory$ action server:

\begin{enumerate}
\item Create a rclpy node that acts as the client for the\\ $/joint\_trajectory\_controller/follow\_joint\_trajectory$ action server. This node should import the necessary ROS libraries and create an instance of the $rclpy.action.ActionClient$ class to communicate with the server.
\begin{minted}[fontsize=\scriptsize, breaklines]{python}
import rclpy
from rclpy.action import ActionClient
from control_msgs.action import FollowJointTrajectory

rclpy.init(args=None)
node = rclpy.create_node('joint_trajectory_controller_client')
client = ActionClient(node, FollowJointTrajectory, '/joint_trajectory_controller/follow_joint_trajectory')
\end{minted}
\item Construct a $FollowJointTrajectory.Goal$ object that contains the desired trajectory for the robot arm. This object should contain a $JointTrajectory$ message, which specifies the joint positions, velocities, accelerations, and time stamps for the trajectory. Here's an example $FollowJointTrajectory.Goal$ object for a simple two-joint robot arm:
\begin{minted}[fontsize=\scriptsize, breaklines]{python}
from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint

goal_msg = FollowJointTrajectory.Goal()
goal_msg.trajectory.joint_names = ['joint1', 'joint2']
point1 = JointTrajectoryPoint()
point1.positions = [0.0, 0.0]
point1.time_from_start = rclpy.time.Duration(seconds=1.0).to_msg()
point2 = JointTrajectoryPoint()
point2.positions = [1.0, 1.0]
point2.time_from_start = rclpy.time.Duration(seconds=2.0).to_msg()
goal_msg.trajectory.points = [point1, point2]
\end{minted}
This $FollowJointTrajectory.Goal$ object specifies a trajectory with two points, where the first point has joint positions of $[0.0, 0.0]$ and occurs 1 second after the start of the trajectory, and the second point has joint positions of $[1.0, 1.0]$ and occurs 2 seconds after the start of the trajectory.
\item Send the $FollowJointTrajectory.Goal$ object to the $/joint\_trajectory\_controller/follow\_joint\_trajectory$ action server using the $send\_goal$ method of the $ActionClient$ object.
\begin{minted}[fontsize=\scriptsize, breaklines]{python}
future = client.send_goal_async(goal_msg)
\end{minted}
\item Wait for the server to complete the trajectory by calling the result method of the Future object returned by the $send\_goal\_async$ method. This method will block until the server reports that the trajectory has been completed or an error occurs
\begin{minted}[fontsize=\scriptsize, breaklines]{python}
rclpy.spin_until_future_complete(node, future)
result = future.result()
\end{minted}
\end{enumerate}
\subsubsection{The /joint\_states topic}\label{ros2-joint-state}
The $/joint\_states$ topic is a standard ROS 2 topic that provides the current joint positions, velocities, and effort (torque or force) values for all joints in a robot. This topic is typically published by a robot's joint state publisher node, which reads the joint positions, velocities, and effort values from the robot's joint sensors and publishes them on the $/joint\_states$ topic.

Here's an example Python script that subscribes to the /joint\_states topic and prints the current joint positions for a robot arm:
\begin{minted}[fontsize=\scriptsize, breaklines]{python}
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import JointState

class JointStateSubscriber(Node):
    def __init__(self):
        super().__init__('joint_state_subscriber')
        self.subscription = self.create_subscription(JointState, '/joint_states', self.joint_state_callback, 10)

    def joint_state_callback(self, msg):
        # Print the current joint positions
        for i, name in enumerate(msg.name):
            position = msg.position[i]
            self.get_logger().info(f"Joint {name} position: {position}")

def main(args=None):
    rclpy.init(args=args)
    node = JointStateSubscriber()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
\end{minted}
This script creates a $JointStateSubscriber$ node that subscribes to the $/joint\_states$ topic and defines a $joint\_state\_callback$ function that prints the current joint positions for each joint in the robot arm. The $create\_subscription$ method of the Node class is used to create a subscription to the $/joint\_states$ topic, and the spin method is called to start the ROS event loop and receive messages from the subscription.

Assuming that the joint state publisher node is running and publishing joint state messages on the $/joint\_states$ topic, running this script will cause the current joint positions for each joint in the robot arm to be printed to the console whenever a new joint state message is received on the $/joint\_states$ topic.

\subsection{Unified Robot Description Format (URDF)}\label{urdf}
Unified Robot Description Format is an XML-based file format used to describe the kinematics and dynamics of a robot for simulation or visualization purposes. The main purpose of URDF is to provide a standardized way to represent robots that can be easily shared and used across different robotics platforms and simulation tools.

Here are some of the main tags used in a URDF file and their brief explanations:
\begin{itemize}
\item \textbf{<robot>:} The root tag of the URDF file that defines the robot and its properties.
\item \textbf{<link>:} Defines the properties of a rigid body link of the robot, such as its name, its visual and collision geometries, and its inertial properties.
\item \textbf{<joint>:} Defines a joint that connects two links together and specifies the type of joint, its name, and its parent and child links.
\item \textbf{<gazebo>:} Defines the properties of the robot for Gazebo, a popular open-source physics engine used for robot simulation. This tag includes sub-tags such as \textit{<plugin>} to specify Gazebo plugins, \textit{<material>} to define material properties, and \textit{<sensor>} to define sensors attached to the robot.
\item \textbf{<transmission>:} Defines the transmission properties of the robot, which specify how the input and output shafts of a joint are connected and how torque, force, or velocity is transmitted between them.
\end{itemize}

\subsection{Simulation Description Format (SDF)}\label{sdf}
Simulation Description Format is a file format used to describe the physical properties and dynamics of objects in a simulation environment. It is often used in robotics and autonomous systems, and is supported by popular simulators such as Gazebo, Ignition, and Webots. Here are the main tags used in SDF:
\begin{enumerate}
\item \textbf{<sdf>:} This is the root tag of the SDF file, and it specifies the version of the SDF format being used.
\item \textbf{<model>:} This tag is used to define a model in the simulation environment. It contains sub-tags such as \textit{<link>},\textit{ <joint>}, and \textit{<plugin>} that define the properties of the model.
\item \textbf{<link>: }This tag is used to define a link in a model. It contains sub-tags such as \textit{<inertial>}, \textit{<collision>}, and \textit{<visual> }that define the physical properties of the link.
\item \textbf{<joint>:} This tag is used to define a joint that connects two links in a model. It contains sub-tags such as \textit{<axis>}, \textit{<dynamics>}, and \textit{<limit>} that define the properties of the joint.
\item \textbf{<plugin>:} This tag is used to define a plugin that provides additional functionality to the simulation. It contains attributes such as name and filename that specify the name and location of the plugin.
\item \textbf{<sensor>:} This tag is used to define a sensor that can be attached to a link in a model. It contains sub-tags such as \textit{<camera>}, \textit{<imu>}, and \textit{<ray>} that define the properties of the sensor.
\item \textbf{<collision>:} This tag is used to define the collision properties of a link. It contains sub-tags such as \textit{<geometry>} and \textit{<surface>} that define the shape and surface properties of the link.
\item \textbf{<visual>:} This tag is used to define the visual properties of a link. It contains sub-tags such as \textit{<geometry>} and \textit{<material>} that define the shape and appearance of the link.
\item \textbf{<inertial>:} This tag is used to define the inertial properties of a link. It contains sub-tags such as \textit{<mass>} and \textit{<inertia>} that define the mass and moment of inertia of the link.
\item \textbf{<geometry>:} This tag is used to define the shape of a link or collision object. It contains sub-tags such as \textit{<box>}, \textit{<cylinder>}, and \textit{<mesh>} that define the shape of the object.
\item \textbf{<material>: }This tag is used to define the appearance of a link or collision object. It contains sub-tags such as \textit{<ambient>}, \textit{<diffuse>}, and \textit{<specular>} that define the color and reflectivity of the object.
\item \textbf{<include>: }This tag is used to include another SDF file into the current SDF file. It contains attributes such as uri that specify the location of the included file.
\end{enumerate}

Overall, SDF is a powerful format that allows developers to describe complex robotic systems with multiple links, joints, sensors, and plugins. It provides a standardized way to simulate and test robots in a variety of environments and scenarios, which can greatly improve the performance and safety of the final product.

\subsection{DAE and STL file formats }\label{dae_stl}
DAE stands for Digital Asset Exchange, and it is a file format used for exchanging 3D digital assets between different software applications. DAE files can contain information about the geometry, materials, textures, animations, and other properties of 3D models. DAE files are supported by many 3D modeling software applications such as Blender \cite{Blender}, Autodesk Maya \cite{Maya}, and SketchUp \cite{SketchUp}.

STL stands for Standard Triangle Language, and it is a file format used for storing 3D models as a series of connected triangles. STL files only represent the surface geometry of a 3D model and do not include information about materials, textures, or other properties. STL files are commonly used for 3D printing and rapid prototyping, as they can be easily sliced into layers and printed using a 3D printer.

Both DAE and STL files are commonly used in 3D modeling and engineering applications. While DAE files are more versatile and can contain a wider range of information about a 3D model, STL files are simpler and more commonly used in the 3D printing industry.

\subsection{RViz, RQt, and RQt Graph}\label{rviz_rqt}
RViz, RQt, and RQt Graph are three useful tools in the ROS ecosystem that allow developers to visualize, interact with, and analyze data from ROS nodes and topics. Here's a brief explanation of each tool:
\begin{enumerate}
\item \textbf{RViz:} RViz is a 3D visualization tool for ROS that allows developers to visualize robot models, sensor data, and other information in a simulated environment. With RViz, developers can view and manipulate 3D models of robots and their surroundings, as well as plot sensor data in real-time. RViz is useful for debugging and testing robot algorithms and behaviors, as well as for creating realistic simulations of robot environments.
\item \textbf{RQt:} RQt is a graphical user interface (GUI) tool for ROS that provides a suite of plugins for analyzing and debugging ROS nodes and topics. RQt plugins allow developers to monitor and visualize ROS topics, plot data, and inspect the internal state of nodes. With RQt, developers can easily interact with the ROS network and analyze the data flowing between nodes.
\item \textbf{RQt Graph: }RQt Graph is a visualization tool for ROS that allows developers to see the ROS network graph and visualize the data flow between nodes. With RQt Graph, developers can see the connections between nodes and topics, as well as the current state of each node (e.g., whether it is running, paused, or stopped). RQt Graph is useful for understanding the overall architecture of a ROS system and for debugging issues with data flow between nodes.
\end{enumerate}

In summary, RViz, RQt, and RQt Graph are three powerful tools in the ROS ecosystem that allow developers to visualize, interact with, and analyze data from ROS nodes and topics. RViz provides 3D visualization capabilities, while RQt provides a suite of plugins for analyzing and debugging ROS nodes and topics. RQt Graph provides a visualization of the ROS network graph and data flow between nodes.


\subsection{DDQN}

Double Deep Q Learning (DDQN) \cite{42_hamdia2021efficient} is a Q-Learning algorithm extension that overcomes Q-value overestimation in standard Q-Learning. DDQN is a DRL method that combines a deep neural network with the Q-Learning technique to develop an optimum policy for an agent to make decisions in an environment.
The Double Deep Q-Learning (DDQN) method is an extension of the classic Q-Learning algorithm, which is widely used in Reinforcement Learning (RL) for robotics and robotic arm control. Deep RL method DDQN combines two deep neural networks to estimate the Q-values of state-action pairings in an environment. In this section, we will go through the DDQN idea, its benefits, and how it may be applied in robotics and robotic arm control.
Q-Learning is a well-known RL method that may be used to identify the best policy for an agent in a given environment. The agent in Q-Learning attempts to learn a function Q(s, a) that calculates the anticipated reward for doing an action in state s. The best policy is then found by choosing the action that maximizes the Q-value for a particular state. Q-Learning, on the other hand, can only be employed in tiny settings with a limited number of states and actions. It is unsuitable for big and complicated situations.
Deep Q-Learning (DQL) is a Q-Learning variant that uses a deep neural network to estimate Q-values for state-action pairings in vast and complicated contexts. DQL has been demonstrated to be successful in a variety of difficult contexts, including video games and robotic control. However, it has been discovered that DQL can occasionally overstate Q-values, resulting in poor strategies.
To overcome this issue, Double Deep Q-Learning (DDQN) was created, which estimates Q-values using two deep neural networks. One network, known as the target network, is used to produce Q-value targets, while the other, known as the online network, is used to generate Q-value estimates. To promote training stability, the Q-value objectives are updated less frequently than the Q-value estimations. This method decreases overestimation and enhances algorithm convergence.
The DDQN algorithm is summarized below:
\begin{enumerate}
\item Set up two deep neural networks, one for the target network and one for the online network.
\item Create a replay buffer to record the agent's experience tuples (state, action, reward, next state).
\item Set the exploration strategy's parameters, such as the epsilon-greedy policy.
\item For every episode:
\begin{enumerate}
\item Return the environment to its original state.
\item Using the exploratory technique, choose an action.
\item Carry out the activity and watch for the reward and the following state.
\item Put the experience tuple into the replay buffer.
\item Take a random sample of experience tuples from the replay buffer.
\item Using the target network, compute the Q-value targets.
\item Using the online network, compute the Q-value estimations.
\item Determine the difference between the Q-value objectives and the Q-value estimations.
\item To minimize loss, update the online network via backpropagation.
\item Periodically update the target network with the weights of the online network.
\item Repeat steps b–j until the episode is finished.
\end{enumerate}
\item Repeat step 4 for a fixed number of episodes or until the agent reaches a satisfactory level of performance.
\end{enumerate}

There are various advantages of using DDQN over standard Q-Learning and DQL. It decreases overestimation, enhances stability, and speeds up convergence. DDQN is also capable of dealing with vast and complicated settings with multidimensional state and action spaces. These characteristics make DDQN an appealing candidate for robotic control applications.
DDQN may be used in robotics and robotic arm control to determine an optimum policy for the agent to complete certain tasks such as item grabbing or assembly. Without any prior understanding of the environment or the work, DDQN may be used to learn the policy from the ground up. The agent may investigate its surroundings and learn from its experiences in order to better its performance.

\subsection{Machine Learning Algorithms}
Machine learning is a branch of AI that entails teaching computers to learn from data without being explicitly programmed. Machine learning seeks to develop models that can make predictions or judgments based on input data. In this post, we will go over the fundamentals of machine learning, such as its many kinds, techniques, and applications. 
There are several machine learning algorithms that are utilized in various sorts of machine learning jobs. Machine learning methods of many types can be applied to robotics and robotic arm control jobs. 

A machine learning algorithm that can be used to operate robotic arms. Here are some examples of how to utilize decision trees in this context:
    \begin{enumerate}
    \item Classification of Arm Configurations: ML Algorithms can be used to categorize various arm configurations depending on sensor data input. A decision tree, for example, may be trained to classify various arm configurations based on the location and orientation of the arm joints.
    \item Grasp Classification: Using sensor data as input, decision trees may be used to categorize several sorts of grasping actions. A decision tree, for example, may be taught to distinguish between power and precision grasps depending on the pressure applied to the item being clutched.
    \item Object Recognition: Using sensor data as input, decision trees can be implemented to recognize various things. A decision tree, for example, may be trained to recognize different things based on their form and size, which could be beneficial for gripping and putting objects in precise spots.
    \item Ideal Path Planning: Using input sensor data and the desired job, decision trees may be utilized to design the ideal path for the robotic arm to travel. A decision tree, for example, may be taught to identify the best path for the arm to take to pick up an object depending on its size and position.
    \end{enumerate}
Some machine learning algorithms and their uses in our context are:
\begin{itemize}
\item \textbf{Decision Trees:} Decision trees are used for classification jobs in which the purpose is to label incoming data.  In robotics, decision trees may be used for a number of tasks like object detection, path planning, and motion control. A decision tree, for example, might be used to identify different sorts of objects in a robot's surroundings or to decide the best course for a robot to take based on sensor data. Decision trees can be used to classify different arm configurations or grab types for robotic arm control.
Based on sensor data input, decision trees may be used to identify various arm configurations or grip kinds. They may also be utilized to forecast the best path for the arm to take in order to complete a certain job.

\item \textbf{Random Forests:} Random forests are an ensemble approach for improving the accuracy and resilience of classification problems by combining several decision trees. Random forests can be used in robotics to increase the accuracy and resilience of classification tasks. A random forest, for example, can be taught to identify different types of terrain based on sensor data input or to classify different sorts of items in a robot's surroundings. They can be used in robotic arm control to identify various items or to forecast the best-grabbing approach for certain things.

\item \textbf{SVMs (Support Vector Machines):} SVMs are used for classification and regression problems. SVMs may be used in robotics for a range of tasks including object detection, path planning, and control. An SVM, for example, can be trained to recognize different sorts of objects based on their shape and size or to predict the best course for a robot to take depending on sensor data input. They can be used in robotic arm control to forecast the best path for the arm to take or to categorize various arm configurations.

\item \textbf{Neural Networks:} A common deep learning approach, neural networks may be used for a range of tasks such as classification, regression, and reinforcement learning. In robotics, neural networks may be utilized for a range of tasks like object identification, motion control, and reinforcement learning. A neural network, for example, may be used to recognize different sorts of objects based on sensor data input or to learn and optimize the control of a robot's movement over time. They may be used in robotic arm control to understand complicated patterns and correlations in arm motions and forecast the best course for the arm to take.

\item \textbf{Clustering algorithms:} these algorithms can be used to group together comparable data points, which is important in robotic arm control for finding similar grasping actions or arm configurations.

\end{itemize}

The choice of machine learning algorithm will depend on the specific task and data available in robotic arm control. It is important to select an algorithm that can effectively learn from the available data and perform the desired task accurately and efficiently. The application of these algorithms can greatly enhance the control and operation of robotic arms in various fields including manufacturing, logistics, and healthcare.

\subsection{Neural Networks}
Neural networks are machine learning models inspired by the structure and function of the human brain. They are effective tools for addressing a wide range of complicated issues, including as image and audio recognition, natural language processing, and gameplay. This section will offer an overview of neural networks, covering their construction, training method, and applications.

\subsubsection{Structure of Neural Networks}
Layers of linked nodes or neurons form neural networks, which are organized into an input layer, one or more hidden layers, and an output layer. Each neuron takes input from neurons in the previous layer, processes it using an activation function, and generates an output signal that is transferred to neurons in the next layer \cite{43_hamdia2021efficient}.
A neural network's input layer accepts raw input data, such as an image or a written document, and routes it to the first hidden layer. Depending on the network's topology, each neuron in the hidden layer analyses this input and creates an output signal, which is then passed on to the next hidden layer or the output layer. The output layer generates the network's final output, which might be a classification label, a numerical value, or a collection of probabilities.

\subsubsection{Training of Neural Networks}
The process of training a neural network entail modifying the weights and biases of the neurons in order to minimize the discrepancy between the network's expected and actual output. This is accomplished using a technique known as backpropagation, which analyses the difference between the expected and actual output and propagates this mistake backward through the network layers to alter the weights and biases of the neurons.
The backpropagation method adjusts the weights and biases of the neurons using an optimization approach such as gradient descent to minimize the error between the expected and actual output. The optimization procedure entails interactively changing the weights and biases of the neurons depending on the computed error until the error is reduced to an acceptable level.

\subsubsection{Types of Neural Networks}
There are several varieties of neural networks, each built for a unique issue or data format. Among the most frequent forms of neural networks are:
\begin{itemize}
\item \textbf{Feedforward neural networks} are the most basic sort of neural network, with information flowing from the input layer to the output layer in just one way.
\item \textbf{Convolutional neural networks (CNNs)} are specialized neural networks developed for image and video processing, with a two-dimensional array of pixels as the input.
\item R\textbf{ecurrent neural networks (RNNs)} are neural networks that are designed to process data sequences such as text or time series data.
\item \textbf{Long short-term memory (LSTM) networks }are particularly developed for processing data sequences with long-term dependencies.
\end{itemize}

\subsubsection{Applications of Neural Networks}
Neural networks have several applications in domains such as computer vision, natural language processing, robotics, and finance. Among the most frequent neural network applications are:
\begin{itemize}
\item \textbf{Image and audio recognition:} Neural networks may be taught to accurately recognize and categorize pictures and sounds.
\item \textbf{Natural Language Processing:} Neural networks may be used to analyze and create natural languages, such as machine translation and text production. Neural networks may be used to regulate and optimize robot motions such as grasping and manipulation.
\item \textbf{Finance:} Neural networks may be used to forecast stock prices, assess credit risk, and detect fraud.

\end{itemize}

\subsubsection{Neural networks in robotics}
Because of their capacity to learn from data and make predictions or judgments based on that data, neural networks are commonly utilized in robotics. They are useful for a wide range of applications including object detection, path planning, motion control, and manipulation. 
Object recognition is one of the most popular applications of neural networks in robotics. Neural networks may be trained on vast datasets of images to recognize distinct objects and categories them. This can be applied to jobs like picking and arranging things, in which a robot must detect and grip an object in a chaotic environment.
Neural networks can additionally be employed for route planning, which is the process of identifying a safe and efficient way for a robot to go from one site to another. By training a neural network on a dataset of maps and obstacle configurations, the network can learn to anticipate the optimum path for the robot to take.
Neural networks can be used in motion control to regulate the movement of robotic joints and end-effectors. By training a neural network on a dataset of joint angles and related end-effector locations, the network may learn to anticipate the joint angles necessary to move the end-effector to a desired position \cite{44_villegas2018neural}.
Manipulation tasks, such as grasping and assembling, are another use of neural networks in robotics. A neural network may learn to anticipate the optimum gripping stance for a particular item by training it on a dataset of grasping poses and matching object attributes.

\subsubsection{Neural Network for the robotic arm}
In robotics, neural networks may be used to control the movement of robotic arms, among other things. One typical way is to employ a neural network as an arm controller, in which the network receives sensor readings and generates control signals to move the arm to a desired position.
A neural network must be trained on a dataset of sensor readings and matching arm positions before it can be used for arms control. The network's inputs might comprise joint angles, velocities, and end-effector locations, with the output being the joint torques necessary to move the arm to the desired position.
The training method generally consists of iteratively modifying the network's weights to minimize the gap between the network's expected and true outputs. This may be accomplished through the use of various optimization techniques, such as stochastic gradient descent. Once trained, the network may be utilized to operate the arm in real time. The network receives sensor values from the arm and predicts the control signals required to move the arm to the desired position. The control signals may then be delivered to the arm's actuators, causing the arm to move. Other tasks connected to robotic arms, such as object identification and grasping, can also be performed using neural networks. A neural network, for example, may be trained on a collection of photographs of items and their related grasping stances and then used to predict the optimal gripping pose for a specific object.

\subsection{Reinforcement Learning Algorithms and The Markov Decision Process (MDP)}
Reinforcement learning (RL) methods are particularly well-suited for robotics applications, such as robotic arm control. RL algorithms learn to control the arm by picking actions that maximize a reward signal, which may be a measure of how successfully the arm performs a task or how far it progresses toward a goal. Here are some popular RL algorithms used in robotics and for operating robotic arms:

\begin{itemize}
\item \textbf{Deep Deterministic Policy Gradient (DDPG):} The Deep Deterministic Policy Gradient (DDPG) is an actor-critic method that is meant to handle continuous action spaces, which is vital for directing the movement of a robotic arm \cite{44_villegas2018neural}. It estimates the action-value function using a deterministic policy function and a critic network and has been effectively applied to tasks such as reaching and grasping.
\item \textbf{Trust Region Policy Optimization (TRPO):} TRPO is a policy optimization algorithm that is well-suited for jobs requiring precise and accurate control, such as managing the position of a robotic arm \cite{45_1_lim2020federated}, \cite{45_kim2020motion}. It employs a trust region strategy to guarantee that policy adjustments are not too significant, which aids in maintaining stability.
\item \textbf{Asynchronous Advantage Actor-Critic (A3C): }A3C is a scalable and efficient RL technique designed to teach robotic arms to do complicated tasks such as item manipulation in crowded surroundings \cite{46_han2023survey}. It explores the state space and updates the policy using numerous simultaneous agents, which can dramatically accelerate the learning process.
\item \textbf{Deep Neural Networks for Q-Learning:} Q-Learning is a value-based RL algorithm that learns to assess the worth of actions in a given state. When Deep Q-Networks (DQN) are merged with deep neural networks, they form Deep Q-Networks (DQN), which are particularly helpful for applications involving high-dimensional sensory input, such as vision-based control of a robotic arm \cite{47_gupta2021deep}.
\item \textbf{Proximal Policy Optimization (PPO):} PPO is a policy optimization algorithm that updates the policy using a clipped surrogate objective function \cite{48_schulman2017proximal}. It has been used effectively in a variety of robotics activities, including regulating the movement of a robotic arm to conduct pick-and-place operations.

\end{itemize}

\subsubsection{The Markov Decision Process (MDP)}
MDP is a framework for modeling decision-making issues when the consequence of an action is unknown \cite{49_delage2010percentile}. It is commonly used in the field of reinforcement learning to simulate issues like robotic arm control, autonomous vehicle navigation, and gameplay. The decision-making agent in a MDP interacts with the environment in discrete time steps. At each time step, the agent performs an action depending on the current state of the environment and is rewarded with a new state. The agent's purpose is to discover a policy that maximizes the cumulative reward over time.
To convert a situation to an MDP framework, we must first identify the MDP components: state space, action space, transition probabilities, and reward function. Here's a quick rundown of each component:
\begin{enumerate}
\item \textbf{The state space} is the collection of all conceivable states in which the environment can exist. The state space of a robotic arm might comprise variables such as the arm's position and orientation, the placement of items in the environment, and the status of any sensors or actuators.
\item \textbf{The action space} is the collection of all conceivable actions that the agent can do in a given condition. The action space for a robotic arm might comprise orders to move the arm in different directions, alter its hold on an item, or activate sensors.
\item \textbf{Transition Probabilities} indicate the possibility of a state changing when a certain action is done. The transition probabilities of a robotic arm can be affected by the physical qualities of the arm and the objects in the surroundings, as well as any noise or uncertainty in the sensors or actuators.
\item \textbf{The reward function} assigns a monetary incentive to each state-action pair that represents the agent's aim. The reward function for a robotic arm can offer a positive reward for successfully gripping an object, a negative reward for colliding with an impediment, and zero otherwise.
\end{enumerate}
Reinforcement learning algorithms use MDPs to learn how to make decisions in a sequential decision-making problem.
These are only a few examples of RL algorithms that have been employed in robotics and robotic arm control. The algorithm used will be determined by the specific job and surroundings, as well as the features of the robot and sensors. RL algorithms offer the potential to make robotic arms more independent and adaptable, allowing them to adapt to changing surroundings and tasks in real-time.

\subsubsection{The exploration and exploitation trade-off in RL}
The exploration-exploitation trade-off in Reinforcement Learning (RL) refers to the issue encountered by an agent when deciding whether to continue exploring the environment to obtain additional knowledge or exploit the information it has already received to maximize its rewards.
Exploration is the process of attempting new behaviors and evaluating their effects in order to understand more about the environment. It entails attempting actions that have never been attempted before or attempting previously attempted actions with a new parameter configuration. Exploitation, on the other hand, refers to the practice of maximizing the cumulative benefit by utilizing knowledge learned from prior acts. It entails doing behaviors that have provided high benefits in the past or are expected to bring high returns in the future.
In RL, the trade-off between exploration and exploitation is crucial because if the agent concentrates just on exploration, it may not get enough incentives to develop an effective strategy. On the other side, if the agent is overly focused on exploitation, it may miss out on learning about other feasible, but lesser-known, positive acts that could contribute to higher long-term benefits.
To balance the exploration-exploitation trade-off, several ways can be applied, including:
\begin{enumerate}
\item\label{epsilon-greedy} \textbf{Epsilon-greedy:} The agent chooses the action with the largest expected payoff with a probability of (1-epsilon) and a random action with a probability of epsilon. This enables the agent to explore the environment while still taking advantage of actions with high expected returns.
\item \textbf{Upper Confidence Bound (UCB):} The action with the highest upper confidence bound is chosen, which is determined by the mean reward and the variance of the reward distribution \cite{50_kaufmann2012bayesian}. This method encourages the agent to investigate less-explored behaviors while continuing to exploit acts with greater predicted rewards.
\item \textbf{Thompson Sampling:} In this method, the agent keeps a probability distribution over the reward of each action and chooses an action based on a sample from that distribution. This technique strikes a balance between exploration and exploitation by choosing activities with a high likelihood of high rewards while also investigating actions with a low probability of high rewards.
\end{enumerate}

\hrule 

In subsequent chapters, you can reference this one to avoid having to explain everything over and over again. This means that you just include things here that are necessary for the understanding of later chapters, nothing more.
\subsection{Topic 01}\label{sec:grundlagen1}
\hrule
%
\section{Implementation}\label{sec:umsetzung}
\subsection{Environment Preparation}
A SDF file containing the object the robotic arm has to learn to touch was created (Figure \ref{fig:can}).
The robotic arm used in the simulation is the Kuka KR210. The necessary files to simulate such a robotic arm were taken from ~\cite{RoboND-Kinematics-Project}. 

The first step is to create a ROS2 workspace and package in which we would place the simulation files, having a folder structure as the one in Figure \ref{fig:project_structure_1}.

\begin{figure}[H]
  \centering
 % \includegraphics[width=0.3\linewidth]{project_structure_1}
  \subfloat[Created SDF file in Gazebo which only contains the object to be touched by the robotic arm.]{\includegraphics[width=0.47\textwidth]{can}\label{fig:can}}
  \hfill
  \subfloat[Project structure after copying the \textit{dae}, \textit{stl}, and \textit{urdf} files into our package.]{\includegraphics[width=0.35\textwidth]{project_structure_1}\label{fig:project_structure_1}}
  \caption{World created in Gazebo and starting directory structure after copying the robotic arm simulation files.}
\end{figure}

With the simulation files in our package, we can use the \textit{ROS robot\_state\_publisher} node and Rviz \ref{rviz_rqt} to visualize the robotic arm, the result is shown in Figure \ref{fig:kuka1}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{kuka1}
	\caption[Rviz2 kuka]{Kuka KR210 in Rviz2.}
	\label{fig:kuka1}
\end{figure}

\textit{ROS robot\_state\_publisher} node takes the \textit{urdf} file as an input and publishes its content to a \textit{/robot\_description} topic to which \textit{Rviz} subscribes to get the \textit{urdf} content and show it (Figure \ref{fig:rosgraph1}). 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{rosgraph1}
\caption{The ROS \textbf{robot\_state\_publisher} node takes the URDF file content and publishes it to the \textbf{/robot\_description} topic to which Rviz subscribes and gets the robotic arm information to finally show it.}
\label{fig:rosgraph1}
\end{figure}

The robotic arm consists of six joints and seven links that connect them. The six joints of the KR210 are numbered J1 to J6, and they allow the robot to move in various directions and orientations.
\begin{itemize}
	\item \textbf{J1:} The first joint is the base joint, which allows the robot to rotate horizontally around its vertical axis.
	
	\item \textbf{J2:} The second joint is the shoulder joint, which allows the robot to lift and lower its arm vertically.
	
	\item \textbf{J3:} The third joint is the elbow joint, which allows the robot to bend its arm vertically.
	
	\item \textbf{J4:} The fourth joint is the wrist roll joint, which allows the robot to rotate its wrist around its vertical axis.
	
	\item \textbf{J5:} The fifth joint is the wrist pitch joint, which allows the robot to tilt its wrist up and down.
	
	\item \textbf{J6:} The sixth joint is the wrist yaw joint, which allows the robot to rotate its wrist horizontally.	 
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{kuka5}
	\caption[kuka kr210 joints]{Joints and axis around which they allow movement in the Kuka KR210 arm.}
	\label{fig:kuka5}
\end{figure}

The arm also has links that connect the joints, including the base, lower arm, upper arm, wrist, and end-effector. These links are designed to provide strength and rigidity to the robot arm while allowing for smooth and precise movement. These links are:

\begin{itemize}
	\item \textbf{Base Link:} This is the fixed part of the robot that is attached to the ground.
	
	\item \textbf{Link 1:} This is the first link that connects the base to joint 1.
	
	\item \textbf{Link 2:} This link connects joint 1 to joint 2.
	
	\item \textbf{Link 3:} This link connects joint 2 to joint 3.
	
	\item \textbf{Link 4: }This link connects joint 3 to joint 4.
	
	\item \textbf{Link 5: }This link connects joint 4 to joint 5.
	
	\item \textbf{End Effector (Link 6):} This is the final link that connects to the tool or object being manipulated.
	
\end{itemize}


\begin{figure}[H]
  \centering
  \subfloat[Links 1, 3 and 5 in the Kuka KR210.]{\includegraphics[width=0.45\textwidth]{kuka6}\label{fig:kuka6}}
  \hfill
  \subfloat[Base Link, Link 2, 4 and 6 in the Kuka KR210.]{\includegraphics[width=0.45\textwidth]{kuka7}\label{fig:kuka7}}
  \caption{Links in the Kuka KR210.}
\end{figure}

After setting up the robotic arm, the following step is to add sensors to it and to configure it to work with ROS 2 and Gazebo.
\begin{itemize}
\item To detect collisions between the robotic arm and the object it should learn to touch, a bumper sensor is added to links 4, 5, and 6. This is done by adding the code in Listing \ref{bumperSensorXml} to the URDF file.

\item A camera is added to the URDF file as well, this is done by adding the code in Listing \ref{cameraSensorXml}

\item In the SDF file, the Gazebo ROS state plugin was added. This is done by adding the code in Listing \ref{gazeboRosStatePluginXml} in the SDF file. This plugin not only allows us to monitor our models' positions and velocities but also modify them programmatically.
\end{itemize}

Once everything mentioned is done we launch our SDF file in Gazebo and spawn the robotic arm into the world to finally have our setup as in Figure \ref{fig:kuka4}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\linewidth]{kuka4}
	\caption[Gazebo World]{Gazebo world containing the robotic arm, the object to be touched, plus the sensors and plugins added.}
	\label{fig:kuka4}
\end{figure}

A launch file was created to spawn everything as in Figure \ref{fig:kuka4}, also the ROS nodes and topics in figures \ref{fig:rosgraph00}, \ref{fig:rosgraph01}, \ref{fig:rosgraph02}, \ref{fig:rosgraph03}, \ref{fig:rosgraph04} are created.


\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{rosgraph00}
\caption{The joint state broadcaster node publishes the current state of each joint in the robot's body to the ROS (Robot Operating System) network. This state includes the joint's position, velocity, and effort (torque) values. }
\label{fig:rosgraph00}
\end{figure}
 
\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{rosgraph01}
\caption{The node \textit{/gazebo\_state/gazebo\_ros\_state} publishes the state of the models in the gazebo world to the topics \textit{/gazebo\_state/link\_states\_demo} and \textit{/gazebo\_state/model\_states\_demo} as specified in Listing \ref{gazeboRosStatePluginXml}}
\label{fig:rosgraph01}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{rosgraph02}
\caption{As a result of adding Listing \ref{cameraSensorXml} a node \textit{camera\_controller} is created and it publishes to the topics \textit{/camera/image\_raw/*}.}
\label{fig:rosgraph02}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{rosgraph03}
\caption{As a result of adding Listing \ref{bumperSensorXml} there are \textit{/contact\_sensor/gazebo\_ros\_bumper\_sensor} nodes publishing the collision details to the topics \textit{/contact\_sensor/bumper\_link\_4}, \textit{/contact\_sensor/bumper\_link\_5}, and\textit{ /contact\_sensor/bumper\_link\_6}. }
\label{fig:rosgraph03}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{rosgraph04}
\caption{The joint trajectory controller node is responsible for generating and executing a trajectory plan for the robot's joints. This is the node used to send the desired positions when controlling the robotic arm}.
\label{fig:rosgraph04}
\end{figure}

With the robotic arm spawn and the nodes and topics running to interact with it, the script to define the environment and the steps to be taken in the experiment is created. 

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{classDiagramComplete}
\caption{Classes present in the python script that runs the environment experiments for the arm to learn to touch the object.}
\label{fig:classdiagramcomplete}
\end{figure}

The main components are:
\begin{itemize}
\item The action space is defined as angle variations of 0.03 radians for the Joints 1, 2 and 3. That is a space of 3 dimensional vectors whose components can only be three possible values (i.e. -0.03, 0, and 0.03).
\item The state space consists of four dimensional vectors whose components are the angle values of Joints 1, 2, and 3, and the distance between Link 6 and the object the arm has to touch.
\item The function computeReward in the EnvironmentManager class takes care of computing the reward, it is executed when the function takeAction in the EnvironmentManager class is executed. The computeReward function is as follows: 

$reward = 
 \begin{cases} 
 	100 	& \text{if the arm touches the object.} \\ 
 	1 		& \text{if episode terminates at step.} \\ 
 	-1 		& \text{if the arm moves away from the object.} 
 \end{cases}$

\item The ReplayMemory class keeps record of experiences, for every step it records the current state, the action taken, the reward, and the new state.
\item The EpsilonGreedyStrategy class provides the exploration probability, at the beginning the exploration probability is high and as the simulation continues this is defined to decay up to a minimum, at which point mostly exploitation is done.
\item The class NN defines the neural network used. The one used has two hidden layers with 32 and 64 nodes and ReLU as the activation function. The input and output layers are defined by the dimension of the state space and the number of actions in the action space respectively.
\item The Agent class takes care of selecting an action based on the exploration probability given by the EpsilonGreedyStrategy class.

\end{itemize} 

\subsection{The Deep Q Learning Algorithm}
Once we have our environment prepared, we are ready to implement the following Deep Q Learning Algorithm:
\begin{enumerate}
\item  \label{itm:dqn_1} Initialize the replay memory buffer $D$ with capacity $N$.
\item Initialize the policy Q-network with random weights $\theta$.
\item Clone the policy Q-network and let that be the target Q-network with weights $\theta^{-} = \theta$.
\item For each episode $e=1,2,...,E$ do the following:
	\begin{enumerate}
	\item Initialize the environment with initial state $s_0$.
	\item For each step $t=1,2,...,T$ do the following:
		\begin{enumerate}
		\item With probability $\epsilon$ select a random action $a_t$, otherwise select $a_t=\arg\max_{a} Q(s_t, a; \theta)$.
		\item Execute action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$.
		\item Store the experience $(s_t, a_t, r_t, s_{t+1})$ in the replay memory buffer $D$.
		\item Sample a mini-batch of experiences $(s_j, a_j, r_j, s_{j+1})$ from the replay memory buffer $D$.
		
		\item Compute the Q-learning target value for each experience $(s_j, a_j, r_j, s_{j+1})$:
		
		 $y_j = 
		 \begin{cases} 
		 r_j & \text{if episode terminates at step } j+1 \\ 
		 r_j + \gamma\max_{a'}Q(s_{j+1},a';\theta^{-}) & \text{otherwise} \end{cases}$.
		
		\item Compute the loss between the predicted Q-value and the target Q-value: $L(\theta) = \frac{1}{B}\sum_{j=1}^B(y_j-Q(s_j,a_j;\theta))^2$.
		\item Update the policy Q-network weights using gradient descent: $\theta \leftarrow \theta - \alpha\nabla_\theta L(\theta)$.
		\item Every $C$ steps update the target Q-network weights: $\theta^{-} \leftarrow \theta$.
		\item Update the current state, set $s_t=s_{t+1}$.
		\end{enumerate}
	\end{enumerate}
\end{enumerate}

\subsection{The Deep Q Network (DQN)}               
PyTorch \cite{pytorch} was used for the neural network, specifically the \textit{nn.Module}. 
The nn package contains a Module class which serves as the foundation for all neural network modules. This means that the used network and its layers will inherit from the Module class. To implement the DQN, the class NN that extends the nn.Module class is created. The DQN will be fed the current state as follows:
\begin{align}
	input &= (\theta_1, \theta_2, \theta_3, d) \label{eq:1}
\end{align}

where, $\theta_1, \theta_2, and \theta_3$ represent the joint angles of the robotic arm, and $d$ represents the distance between the end effector of the arm and the object. 

A simple DQN with two fully connected layers and an output layer is used.
\begin{listing}[htbp]
\caption{Used Deep Q Neural Network (DQN).}
\label{code:nn_class_1}
\begin{minted}[fontsize=\scriptsize]{python}
 class NN(nn.Module):
     def __init__(self, nInputs=4, nOutputs=27):
         super().__init__()
         # input current angles and distance
         self.fc1 = nn.Linear(nInputs, out_features=32)
         self.fc2 = nn.Linear(in_features=32, out_features=64)
         self.out = nn.Linear(in_features=64, out_features=nOutputs)
\end{minted}
\end{listing} 

In listing \ref{code:nn_class_1} The fully connected layers are referred to as $Linear$ layers in PyTorch.

The first Linear layer will accept input with dimension of 4. This first layer will generate 32 outputs, which will serve as the input for the second Linear layer. The second Linear layer will have 64 outputs, and the output layer will have 27 outputs (as there are 27 possible actions as in Box 1), taking 64 inputs from the previous layer.

\begin{tcolorbox}[label=mybox, titlebox=visible, title=Box 1. Possible actions that the agent can take]
$A$ = \{(-0.03, -0.03, -0.03), (-0.03, -0.03, 0), (-0.03, -0.03, 0.03),
 (-0.03, 0, -0.03), (-0.03, 0, 0), (-0.03, 0, 0.03), 
 (-0.03, 0.03, -0.03), (-0.03, 0.03, 0), (-0.03, 0.03, 0.03),
  (0, -0.03, -0.03), (0, -0.03, 0), (0, -0.03, 0.03), 
  (0, 0, -0.03), (0, 0, 0), (0, 0, 0.03), (0, 0.03, -0.03), 
  (0, 0.03, 0), (0, 0.03, 0.03), (0.03, -0.03, -0.03), 
  (0.03, -0.03, 0), (0.03, -0.03, 0.03), (0.03, 0, -0.03), 
  (0.03, 0, 0), (0.03, 0, 0.03), (0.03, 0.03, -0.03), 
  (0.03, 0.03, 0), (0.03, 0.03, 0.03)\}
\end{tcolorbox}

The network outputs \textit{Q-values} that correspond to each possible action that the agent can take from a given state. Note that the network used does not have convolutional layers because no image processing takes place.
The final step in defining the DQN class is to create a function called $forward()$. This function will carry out a forward pass through the network. It's important to keep in mind that $forward()$ is a necessary function for all PyTorch neural networks.

\begin{listing}[htbp]
\caption{Implementation of the $forward()$ function needed for the DQN.}
\label{code:nn_class_2}
\begin{minted}[fontsize=\scriptsize]{python}
def forward(self, s):
    s = F.relu(self.fc1(s))
    s = F.relu(self.fc2(s))
    s = self.out(s)
    return s
\end{minted}
\end{listing} 
When the network receives a state $s$; it passes it to the first fully connected layer and applies $relu$ to the output before sending it to the second fully connected layer. After that, $relu$ is applied before passing the result to the output layer. The $forward()$ function then returns the result obtained from the output layer.

\subsection{The Replay Memory}
To define the ReplayMemory class, the Experience class which defines $Experience=(s_j, a_j, r_j, s_j+1)$ is used. The capacity of the Replay Memory is the only parameter needed when creating it.
\begin{listing}[htbp]
\caption{Implementation of the Replay Memory class.}
\label{code:replay_memory}
\begin{minted}[fontsize=\scriptsize]{python}
class ReplayMemory:
    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity)
    def push(self, experience):
        self.memory.append(experience)
    def sample(self, batchSize):
        return random.sample(self.memory, batchSize)
    def canSample(self, batchSize):
        return len(self.memory) >= batchSize
\end{minted}
\end{listing} 

Using the code in Listing \ref{code:replay_memory}, ReplayMemory's capacity is initialized to $capacity$, and the $memory$ attribute is defined as a \textbf{deque} \ref{python:deque}. To add and store $experiences$ the $push$ function is defined, which is just a wrapper for the append built-in function in. When the number of experiences in $memory$ reaches $capacity$, new experiences are kept and old ones are removed. The $sample()$ function provides a batch of random experiences which is used to train the DQN. Finally, the function $canSample()$ tells us whether we can sample from memory or not.

\subsection{The Epsilon Greedy Strategy}
To balance exploration and exploitation the epsilon greedy strategy is used. An exploration rate called $epsilon$ is defined. Epsilon is the probability that the agent will choose a random action (exploration). An $epsilon$ value of 1 means that the agent will explore the environment. As the agent learns $epsilon$ decays by a defined decay rate meaning that the agent knows more about the environment.
\begin{listing}[H]
\caption{Implementation of the EpsilonGreedyStrategy class.}
\label{code:epsilon-greedy}
\begin{minted}[fontsize=\scriptsize]{python}
class EpsilonGreedyStrategy:
    def __init__(self, start, end, decay):
        self.start = start
        self.end = end
        self.decay = decay
    def getExplorationRate(self, currentStep):
        return self.end + (self.start - self.end) * math.exp(-1. * currentStep * self.decay)
\end{minted}
\end{listing} 
In Listing \ref{code:epsilon-greedy}, $start$, $end$, and $decay$ correspond to the starting, ending, and decay rate for $epsilon$. The function $get_exploration_rate()$ has the $currentStep$ of the agent as parameter and returns the calculated exploration rate.

\subsection{The Agent}
The implemented Agent class in Listing \ref{code:agent} has a $strategy$ and $nActions$ as inputs. So, an instance of the EpsilonGreedyStrategy class, is needed to create the agent. The $nActions$ parameter refers to the number of possible actions that the agent can take from a given state. In this case, this number is always be twenty seven as all possible actions are written in Box 1.

\begin{listing}[H]
\caption{Implementation of the Agent class.}
\label{code:agent}
\begin{minted}[fontsize=\scriptsize]{python}
class Agent:
    def __init__(self, strategy, nActions):
        self.strategy = strategy
        self.nActions = nActions
        self.currentStep = 0
    def selectAction(self, state, policyNetwork):
        rate = self.strategy.getExplorationRate(self.currentStep)
        self.currentStep += 1
        if rate > random.random():  # explore
            action = torch.tensor([random.randrange(self.nActions)])
        else:  # exploit
        	with torch.no_grad():
            	action = policyNetwork(state).argmax(dim=1)
        return action
\end{minted}
\end{listing} 

The parameter $currentStep$ is set to zero at the beginning and indicates the current step of the agent in the environment.

The policy network refers to a deep Q-network that is trained to learn the optimal policy. In the $selectAction()$ function, the rate variable is set to the exploration rate returned from the epsilon greedy strategy that was passed in when creating the agent, and the $currentStep$ attribute of the agent is incremented by 1. 
Then, we check whether the exploration rate is greater than a random number generated between 0 and 1. If it is, we explore the environment by randomly selecting an action from our action space $A$. If not, we exploit the environment by selecting the action that corresponds to the highest Q-value output from our policy network for the given state. To perform inference, the $torch.no\_grad()$ method is used to turn off gradient tracking since the model is only used for prediction, not training. During training, PyTorch tracks all the forward pass calculations that occur within the network. By turning off gradient tracking,  PyTorch does not to keep track of any forward pass calculations.

\subsection{The Environment Manager}

Finally the class that manages the environment is the EnvironmentManager class. This class implements the $rclpy.node.Node$ class in Python to create a node and interact with the other nodes in the environment.
The following properties are initialized when the EnvironmentManager class is instantiated:
\begin{enumerate}
\item The $done$ property indicates whether the episode has finished or not. An episode ends when there is a collision between the end effector of the arm and the object or when the episode has reached its maximum defined number of steps.
\item\label{env-manager:distance} The $distance$ property stores the latest distance between the end effector of the arm and the object.
\item\label{env-manager:arm-coordinates} The $armX$, $armY$, and $armZ$ properties store the latest $(x, y, z)$ coordinates of the end effector of the arm.
\item The $joint1State$, $joint2State$, $joint3State$, $joint4State$, $joint5State$, and $joint6State$ properties store the latest angles of the arm joints of the same name. 
\item The $joint1Max$, $joint1Min$, $joint2Max$, $joint2Min$, $joint3Max$, and $joint3Min$ properties define the maximum each joint can move without causing collisions within the arm or with the floor. These values were found using Rviz \ref{rviz_rqt}.
\item The $tfBuffer = tf2\_ros.Buffer()$ and

$tfListener = tf2\_ros.TransformListener(self.tfBuffer, self)$ properties are defined in order to calculate the $armX$, $armY$, and $armZ$ properties in ~\ref{env-manager:arm-coordinates}.
\item\label{env-manager:angle-diff} The $angleDiff$ property defines how much each joint's angle can vary per step per episode in radians.
\item The $anglesVariation$ property defines the possible movements for each joints. Each joint can move $-angleDiff$, $0$, or $+angleDiff$ rads.
\item The $actions$ property stores all the possible actions in the action space $A$
\item The $nActions$ property stores the number of actions in the action space $A$. The action space is defined as:
\begin{equation}
  \begin{aligned}
A = \{(-\theta, -\theta, -\theta), (-\theta, -\theta, 0), (-\theta, -\theta, \theta), (-\theta, 0, -\theta), (-\theta, 0, 0), (-\theta, 0, \theta),\\
 (-\theta, \theta, -\theta), (-\theta, \theta, 0), (-\theta, \theta, \theta), (0, -\theta, -\theta), (0, -\theta, 0), (0, -\theta, \theta),(\theta, \theta, -\theta),\\
 (\theta, -\theta, -\theta), (\theta, -\theta, 0), (\theta, -\theta, \theta), (\theta, 0, -\theta), (\theta, 0, 0), (\theta, 0, \theta),(\theta, \theta, 0),\\
 (0, 0, -\theta), (0, 0, 0), (0, 0, \theta), (0, \theta, -\theta), (0, \theta, 0), (0, \theta, \theta), (\theta, \theta, \theta)\} \label{eq:action-space}
  \end{aligned}
\end{equation}
where $\theta$ is the $angleDiff$ defined in ~\ref{env-manager:angle-diff}.
\item\label{env-manager:model-client} The $modelClient$ property is a service client for the \textit{gazebo\_state/set\_entity\_state} service, it is used to set the object's position in the simulated world. 
\item\label{env-manager:client-get-state} The $modelClientGetState$ is a service client for the \textit{gazebo\_state/get\_entity\_state} service, it is used to obtain the object's position programmatically.
\item The properties $modelX$, $modelY$, and $modelZ$ store the $(x, y, z)$ coordinates of the object. When the environment manager class is instantiated the $modelClientGetState$ in ~\ref{env-manager:client-get-state} property is used to obtain these values.
\item\label{env-manager:trajectory-action-client} The property $trajectoryActionClient$ is an action client. It is used to send messages to the $/joint\_trajectory\_controller/follow\_joint\_trajectory$ server in order to move the arm. The messages are of type $FollowJointTrajectory$ as in \ref{ros2-joint-trajectory-controller}.
\item\label{env-manager:model-subscription} The property $modelSubscription$ defines a subscription to the $/gazebo_state/model_states_demo$ topic and is used to check the position of the object programmatically.
\item\label{env-manager:joint-state-subscription} The property $jointStateSubscription$ is a subscription to the $/joint\_states$ topic. It is created and used later as in \ref{ros2-joint-state}
\item The property $contactSensorSubscription$ is a subscription to the collision sensor topic. 
\item The property $collision$ indicates if there is a collision in the simulated environment.
\end{enumerate}

The $EnvironmentManager$ class implements the following methods :
\begin{enumerate}
\item The $jointStateSubscriptionCallback()$ is a callback defined when creating the property $jointStateSubscription$ \ref{env-manager:joint-state-subscription}. This method calculates the $distance$ property in \ref{env-manager:distance}. This method is executed every time a new message is received in the $/joint\_states$ topic. That is, every time the arm moves.
\item The $modelSubscriptionCallback$ is a callback defined when creating the property $modelSubscription$ \ref{env-manager:model-subscription}. This method is executed every time a new message is received in the $/gazebo\_state/model\_states\_demo$ topic. It is used to obtain the position of the object in the world.
\item\label{env-manager:move-arm-to-position} The $moveArmToPosition(self, positions)$ function is used to move the arm to a certain position. The argument $positions$ is the end position of the joints. It works by using the $trajectoryActionClient$ property in \ref{env-manager:trajectory-action-client}.
\item\label{env-manager:move-model-to-position} The $moveModelToPosition(self, model, x, y, z)$ method is used to move the object to a certain position $(x, y, z)$. It works by using the $modelClient$ property in \ref{env-manager:model-client}.
\item The $resetEnvironment$ method moves the arm to the starting position $positions = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$ using \ref{env-manager:move-arm-to-position}, moves the object to the starting position by using \ref{env-manager:move-model-to-position}, and sets the $done$ and $collision$ property to false. 
\item The $contactSensorCallback4$, $contactSensorCallback5$, and $contactSensorCallback6$ methods are executed when a collision between the Links 4, 5 o 6 and the object happens respectively.
\item The $getState$ method returns the current state of the environment. The state is composed of the first three joints angles and the distance between the end effector or Link 6 and the object. 
\item The $takeAction(actionIndex)$ method takes the $actionIndex$ parameter which identifies the action to be taken. Its responsibility is to move the arm according to the action given and returns the $reward$ which is consequence of the given action.
\item The $computeReward(previousDistance)$ computes the reward in the current state given the $previousDistance$. The $previousDistance$ is compared with the current distance between the End Effector or Link6 and the object to know if the arm is getting closer to the object or not. The reward is computed as follows:\\

$reward = 
 \begin{cases} 
 	100 	& \text{if the arm touches the object.} \\ 
 	1 		& \text{if episode terminates at current step.} \\ 
 	-1 		& \text{if the arm moves away from the object.} 
 \end{cases}$
\end{enumerate}


% The $ are not needed here since the align environment is in math mode.



\subsection{Moving the robotic arm}
The AnglesPublisher node was created \ref{mybox:1} to move the arm by publishing the target angles for the joints. To do this the Joint Trajectory Controller is used, this controller generates and executes trajectories for the robot joints. It subscribes to a JointTrajectory message that specifies the desired trajectory, and then generates a control signal to move the robot's joints along the trajectory. By publishing messages of type JointTrajectory to the joint\_trajectory topic the arm is moved.

\subsection{Getting Images from the camera}
The CameraSubscriber node was created to get images from the environment. The plugin previously used takes care of publishing the images to the topic /camera/image\_raw, the CameraSubscriber just subscribes to this topic and gets the images as messages of type Image.



ROS control module is used to move the arm. ROS Control is a framework for building and controlling robots in ROS (Robot Operating System). It provides a standardized way to manage hardware interfaces, controllers, and state machines, making it easier to develop and integrate robot applications.

The Joint State Broadcaster is a ROS node that broadcasts the state of robot joints. It reads joint positions, velocities, and efforts from a robot's hardware interfaces or controllers and publishes them as a JointState message on a ROS topic. This message contains the current state of all joints in the robot's model, allowing other ROS nodes to subscribe and use this information.

The Joint Trajectory Controller is a ROS controller that generates and executes trajectories for robot joints. It subscribes to a JointTrajectory message that specifies the desired trajectory, and then generates a control signal to move the robot's joints along the trajectory. It can use different algorithms to generate this control signal, such as PID, computed-torque, or model-predictive control. This controller is often used in combination with the Joint State Broadcaster to provide closed-loop control of the robot's joints.

Overall, the Joint State Broadcaster and Joint Trajectory Controller are two important components of ROS Control that enable the control of robot joints. The Joint State Broadcaster provides the current state of the joints to other ROS nodes, while the Joint Trajectory Controller generates and executes trajectories for the joints based on desired goals. Together, these components provide a powerful toolset for controlling robot motion in ROS. 








Should refer, where possible, to the preceding chapter, e.e.:
Singular value decomposition of the matrix $\Sigma$ is conducted as explained in Sec.~\ref{sec:grundagen1} using the \textit{lapack} library (see Sec.~\ref{sec:grundlagen2}).

For software development: what is the logic of the developed code, which of it was done by yourself? Sequence diagrams or UML are good tools here.

Please give code snippets only if they take up less that 0.25 pages, and only if it is unavoidable. Longer snippets go to the appendix and are referenced like this: see App.~\ref{Snippet}.

\section{Experiments}\label{sec:exp}



\par\noindent\rule{\textwidth}{0.4pt}
Show here that the goals from the introduction were achieved (or not achieved), you need at least one experiment per goal. Use screenshots, diagrams, plots, photos, etc. as necessary.

\section{Discussion}
2-3 pages are a good idea here. Picks up goals from the introduction (see \ref{sec:ziele}) and experiments (see \ref{sec:exp}) and explains what was achieved and what was not (and why not in this case). Compares results with results from related work, see Sec.~\ref{sec:relwork}. Draws a preliminary conclusion for the whole thesis.
\begin{tcolorbox}[label=mybox]
This is some text.
\end{tcolorbox}

As we can see in box \ref{mybox}, ...
\section{Conclusion}
Give an executive summary for important decision makers here, as well as an outlook (what would you do if you had another 3 months). 2-3 pages are ok here.

\section{Using LaTeX, erase this chapter later}
%
I was too lazy to translate this, it will be translated later. But I believe the ideas are clear!
%
\subsection{Mathematische Gleichungen}
Eine mehrzeilige Gleichung sieht so aus (die Symbole nach den und-Zeichen werden untereinander gesetzt). Die nonmber-Befehle verhindern dass die Gleichung nummertiert wird (Geschmackssache, ist nie falsch wenn eine Gleichung nummeriert ist). Aber: eine Gleichung auf die man refernziert (also die ein Label hat), muss nummeriert sein!
\begin{align}
    A &= \sum_{i=1}^N x_i \label{eq:1}\nonumber\\
    B &= \frac{\pi}{2}
\end{align}

Eine inline-Gleichung: $x=45b + \frac{2}{3}\pi$. Der Text geht weiter! Auf inline-Gleichungen kann man keine Refernzen erstellen.

\subsection{Das ist eine Auflistung}
\begin{enumerate}
\item Element 1
\item Element 2
\end{enumerate}

\subsection{Das ist eine Bullet-Liste}
\begin{itemize}
\item Element 1
\item Element 2
\end{itemize}


\subsection{Eine Grafik bindet man so ein}
Zulässige Formate sind generell eps, pdf und png.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{logo.pdf}
    \caption{Logo der HAW Fulda}
    \label{fig:bildchen}
\end{figure}

\subsection{So schreibt man einen Algorithmus}

\begin{algorithm}[H]
 \KwData{this text}
 \KwResult{how to write algorithm }
 initialization\;
 \While{not at end of this document}{
  read current\;
  \eIf{understand}{
   go to next section\;
   current section becomes this one\;
   }{
   go back to the beginning of current section\;
  }
 }
 \caption{How to write algorithms\label{alg:dummy}
 }
\end{algorithm}

\subsection{So gestaltet man eine Tabelle}

\begin{table}[H]
\caption{Beispielstabelle\label{tab:beispiel}
}
\centering
\begin{tabular}{llr}
\hline
A    & B & C \\
\hline
D      & per gram    & 11.65      \\
          & each        & 1.01       \\
E       & stuffed     & 32.54      \\
F       & stuffed     & 73.23      \\
G & frozen      & 8.39       \\
\hline
\end{tabular}
\end{table}


\subsection{Interne Referenzen}
So wird ein Kapitel oder Unterkapitel referenziert: Kap.~\ref{sec:einleitung},
Kap.~\ref{sec:webquellen}. Auf Gleichungen bezieht man sich so: Wie in Gl.~(\ref{eq:1}) gezeigt,
sehen Gleichungen in der Regel gut aus. Auf Abb.~\ref{fig:bildchen} bezieht man sich so. Auf
Tab.~\ref{tab:beispiel} referenziert man so. Algorithmen sind analog: siehe Alg.~\ref{alg:dummy}.
Generell kann man alles zitieren was ein Label hat.

\subsection{Textformatierung}
\textbf{So wird dick geschrieben} und \textit{so kursiv}.

\subsection{Zitieren}\label{sec:zitate}
Generell zitiert man so: wie in \cite{clemen1989combining} gezeigt, blablaba. Für jedes zitierte Werk ist ein BibTex-Eintrag nötig! Eine gute Quelle ist Google Scholar!!

\subsection{Webquellen zitieren}\label{sec:webquellen}
So wird eine Webquelle zitiert: \cite{shiny1}, siehe auch den Eintag im BibTeX-File.
Wichtig: für jede Web-Quelle ein BibTeX-Eintrag! Wenn Sie das auf die hier gezeigte Art machen, werden URLs (fast) automatisch getrennt. Kontrollieren Sie trotztdem die Literaturliste, es kann sein dass das nicht immer funktioniert.

\subsection{Literaturverzichnis erstellen}
Hierzu müssen BibTeX-Einträge in die Datei literatur.bib eingefügt werden. Die BibTeX-Keys sind jeweils Argumente für die cite-Kommandos! Wenn Sie literatur.bib ändern müssen Sie alles mindestens 5x compilieren: 3x mit latex, 1x mit BibTex und dann noch 2x mit LaTeX (in der Reihengfolge). Am besten Sie machen ein Skript dafür!

%\subsection{Erstellung eines PDFs im PDF-A Format}
%Durch Einbinden geeigneter Packages (pdfx) wird diese Vorlage bereits als PDF-A erzeugt. Sie sollten allerdings die Metadaten in der Datei main.xmpdata anpassen!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Literaturverzeichnis wird
%% automatisch eingefügt
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\bibliographystyle{unsrt}
\bibliography{literatur}

%\lhead{}
%\printbibliography
%\addcontentsline{toc}{section}{\bibname}
\appendix
\section{Code Snippets}
\begin{listing}[htbp]
\begin{minted}[fontsize=\footnotesize,linenos=false,escapeinside=||,samepage]{xml} 
  <gazebo reference="link_4">
    <!-- contact sensor -->
    <sensor name="end_effector_sensor" type="contact">
      <selfCollide>true</selfCollide>
      <alwaysOn>true</alwaysOn>
      <update_rate>500</update_rate>
      <contact>
        <collision>link_4_collision</collision>
      </contact>
      <!-- gazebo plugin -->
      <plugin name="gazebo_ros_bumper_sensor" filename="libgazebo_ros_bumper.so">
        <ros>
          <namespace>contact_sensor</namespace>
          <remapping>bumper_states:=bumper_link_4</remapping>
        </ros>
        <frame_name>link_4</frame_name>
      </plugin>
    </sensor>
  </gazebo>
\end{minted}
\caption{Bumper Sensor}
\label{bumperSensorXml}
\end{listing}

\begin{listing}[htbp]
\begin{minted}[fontsize=\footnotesize,linenos=false,escapeinside=||,samepage]{xml} 
  <gazebo reference="camera_link">
    <material>Gazebo/Black</material>
    <sensor name="camera" type="camera">
      <pose>0 0 0 0 0 0</pose>
      <visualize>true</visualize>
      <update_rate>10</update_rate>
      <camera>
        <horizontal_fov>1.089</horizontal_fov>
        <image>
          <format>R8G8B8</format>
          <width>640</width>
          <height>480</height>
        </image>
        <clip>
          <near>0.05</near>
          <far>8.0</far>
        </clip>
      </camera>
      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
        <frame_name>camera_link_optical</frame_name>
      </plugin>
    </sensor>
  </gazebo>
\end{minted}
\caption{Camera Sensor}
\label{cameraSensorXml}
\end{listing}

\begin{listing}[htbp]
\begin{minted}[fontsize=\footnotesize,linenos=false,escapeinside=||,samepage]{xml} 
<plugin name='gazebo_ros_state' filename='libgazebo_ros_state.so'>
  <ros>
    <namespace>/gazebo_state</namespace>
    <argument>model_states:=model_states_demo</argument>
    <argument>link_states:=link_states_demo</argument>
  </ros>
  <update_rate>1.0</update_rate>
</plugin>
\end{minted}
\caption{Gazebo ROS state plugin}
\label{gazeboRosStatePluginXml}
\end{listing}

\begin{listing}[htbp]
\begin{minted}[fontsize=\footnotesize,linenos=false,escapeinside=||,samepage]{xml} 
  <gazebo>
    <plugin filename="libgazebo_ros2_control.so" name="gazebo_ros2_control">
      <robot_sim_type>gazebo_ros2_control/GazeboSystem</robot_sim_type>
      <parameters>/home/ros/ros2-projects/my-workspace/src/kuka_kr210/config/jtc.yaml</parameters>
    </plugin>
  </gazebo>
\end{minted}
\caption{Gazebo plugin and ROS2 controller configuration file.}
\label{gazeboPluginControllerConfigFile}
\end{listing}


\begin{mdframed}[linecolor=black, topline=false, bottomline=false,leftline=false, rightline=false, backgroundcolor=white]
\begin{minted}[fontsize=\small,linenos=false,escapeinside=||,]{xml} 
  <ros2_control name="GazeboSystem" type="system">
    <hardware>
      <plugin>gazebo_ros2_control/GazeboSystem</plugin>
    </hardware>
    <joint name="joint_1">
      <command_interface name="position">
        <param name="min">-3.14</param>
        <param name="max">3.14</param>
      </command_interface>
      <command_interface name="velocity">
        <param name="min">-3.15</param>
        <param name="max">3.15</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
      <state_interface name="effort"/>
      <param name="initial_position">0.0</param>
    </joint>
    <joint name="joint_2">
      <command_interface name="position">
        <param name="min">-3.14</param>
        <param name="max">3.14</param>
      </command_interface>
      <command_interface name="velocity">
        <param name="min">-3.15</param>
        <param name="max">3.15</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
      <state_interface name="effort"/>
      <param name="initial_position">-1.57</param>
    </joint>
    <joint name="joint_3">
      <command_interface name="position">
        <param name="min">-3.14</param>
        <param name="max">3.14</param>
      </command_interface>
      <command_interface name="velocity">
        <param name="min">-3.15</param>
        <param name="max">3.15</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
      <state_interface name="effort"/>
      <param name="initial_position">0.0</param>
    </joint>
    <joint name="joint_4">
      <command_interface name="position">
        <param name="min">-3.14</param>
        <param name="max">3.14</param>
      </command_interface>
      <command_interface name="velocity">
        <param name="min">-3.2</param>
        <param name="max">3.2</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
      <state_interface name="effort"/>
      <param name="initial_position">-1.57</param>
    </joint>
    <joint name="joint_5">
      <command_interface name="position">
        <param name="min">-3.14</param>
        <param name="max">3.14</param>
      </command_interface>
      <command_interface name="velocity">
        <param name="min">-3.2</param>
        <param name="max">3.2</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
      <state_interface name="effort"/>
      <param name="initial_position">0.0</param>
    </joint>
    <joint name="joint_6">
      <command_interface name="position">
        <param name="min">-3.14</param>
        <param name="max">3.14</param>
      </command_interface>
      <command_interface name="velocity">
        <param name="min">-3.2</param>
        <param name="max">3.2</param>
      </command_interface>
      <state_interface name="position"/>
      <state_interface name="velocity"/>
      <state_interface name="effort"/>
      <param name="initial_position">0.0</param>
    </joint>
  </ros2_control>
\end{minted}
\end{mdframed}

%\begin{figure}
%    \centering
%    %\includegraphics{s}
%    \caption{Normalerweise bindet man Snippets als Bilder ein...\label{Snippet}
%    }
%\end{figure}

\section{Thesis defence}
The defence is 15/20 minutes for Bachelor/Master, followed by questions and a discussion. Both examiners are present, and you can invite external persons since defences are generally public.

Targetted group are non-computer scientists, e.g., from higher management, NOT the examiners. Means that at least $\frac 1 3$ if the presentation is introduction/context/problem statement. You should re-use text/images/graphs/etc from the corresponding chapters here!

1 Slide per minute is a good guideline. If you can guess that some questions are going to be asked anyway, prepare some slides specifically for these questions, makes a good impression, and you can show them in the discussion time, not during the 15 minutes of the presentation.

Defences are not graded, you can only pass or not pass. 

Students are responsible for finding dates for the defence and coordinating this with both supervisors. 

Some common advice is:
\begin{itemize}
    \item Speak slowly and loadly
    \item If you do not have enough time left for all slides, leave some out rather than rushing through all of them!!
    \item Slide numbers!
    \item In presence: be there 10 minutes ahead of time to check projectors etc. Makes a very bad impression if this is not working. Same for online presentations: be there 5 minutes ahead of time to verify screen sharing works.
    \item do not read text from the slides. These should contains key words only, and you explain the rest in free presentation
    \item Defences can by all means be online, more convenient for companies
    \item in presence: always carry a USB key with a PDF of your slides. If you have to use another PC than yours, PowerPoint slides may look very differently (fonts, page setup etc.)
    \item No-Go: spelling errors on slides!!!
    \item Do not use animations, they may not work in an online setting
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Eidesstattliche Erklärung
%% muss angepasst werden
%% in Erklaerung.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Erklaerung.tex}

\section{Extras}
\subsection{Markov Decision Process}
A Markov Decision Process (MDP) is defined by a tuple $<S, A, P, R, \gamma>$, where:

\begin{itemize}
\item $S$ is the set of states in the environment
\item $A$ is the set of actions that can be taken in each state
\item $P$ is the state transition probability matrix, where $P_{ss'}^a = \mathbb{P}[S_{t+1} = s' \mid S_t = s, A_t = a]$
\item $R$ is the reward function, where $R_s^a = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a]$
\item $\gamma$ is the discount factor, where $\gamma \in [0, 1]$
\end{itemize}

The goal of an agent in a Markov Decision Process is to find a policy $\pi: S \rightarrow A$ that maximizes the expected discounted reward:

\begin{equation*}
V_\pi(s) = \mathbb{E}\pi\Bigg[\sum{k=0}^\infty \gamma^k R_{t+k+1} \Bigg| S_t = s\Bigg]
\end{equation*}

or the corresponding action-value function:

\begin{equation*}
Q_\pi(s, a) = \mathbb{E}\pi\Bigg[\sum{k=0}^\infty \gamma^k R_{t+k+1} \Bigg| S_t = s, A_t = a\Bigg]
\end{equation*}

\subsection{Q learning Algorithm}
The Q-learning algorithm is an off-policy temporal difference learning algorithm for finding the optimal action-value function $Q^(s, a)$ in a Markov Decision Process (MDP). The algorithm updates an estimate of $Q^(s, a)$ by iteratively applying the following update rule:

\begin{equation*}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha\Big[ R_{t+1} + \gamma\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \Big]
\end{equation*}

where $s_t$ and $a_t$ are the current state and action, $R_{t+1}$ is the reward received after taking action $a_t$ in state $s_t$ and transitioning to state $s_{t+1}$, $\alpha$ is the learning rate, and $\gamma$ is the discount factor.

The Q-learning algorithm can be summarized as follows:

\begin{enumerate}
\item Initialize the Q-value function $Q(s, a)$ for all state-action pairs.
\item Observe the current state $s_t$.
\item Choose an action $a_t$ based on a policy, such as $\epsilon$-greedy or softmax.
\item Take the action $a_t$ and observe the next state $s_{t+1}$ and reward $R_{t+1}$.
\item Update the Q-value function using the update rule: $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha\Big[ R_{t+1} + \gamma\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \Big]$.
\item Set $s_t = s_{t+1}$ and repeat from step 3 until the end of the episode or termination of the task.
\end{enumerate}

The Q-learning algorithm is guaranteed to converge to the optimal action-value function $Q^*(s, a)$ under certain conditions, such as the MDP being finite and the learning rate $\alpha$ decaying over time.

\subsection{Deep Q Learning}
Deep Q-learning is a variant of the Q-learning algorithm that uses a deep neural network to approximate the action-value function $Q(s, a)$ in a Markov Decision Process (MDP). The algorithm combines reinforcement learning with deep neural networks to enable learning in high-dimensional and continuous state spaces.

The Deep Q-learning algorithm can be summarized as follows:

\textbf{Algorithm: Deep Q-learning}

\begin{enumerate}
\item Initialize the replay memory buffer $D$ with capacity $N$.
\item Initialize the Q-network with random weights $\theta$.
\item Initialize the target Q-network with weights $\theta^{-} = \theta$.
\item For each episode $e=1,2,...,E$ do the following:
\begin{enumerate}
\item Initialize the environment with initial state $s_0$.
\item For each step $t=1,2,...,T$ do the following:
\begin{enumerate}
\item With probability $\epsilon$ select a random action $a_t$, otherwise select $a_t=\arg\max_{a} Q(s_t, a; \theta)$.
\item Execute action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$.
\item Store the transition $(s_t, a_t, r_t, s_{t+1})$ in the replay memory buffer $D$.
\item Sample a mini-batch of transitions $(s_j, a_j, r_j, s_{j+1})$ from the replay memory buffer $D$.

\item Compute the Q-learning target for each transition $(s_j, a_j, r_j, s_{j+1})$:

 $y_j = \begin{cases} r_j & \text{if episode terminates at step } j+1 \\ r_j + \gamma\max_{a'}Q(s_{j+1},a';\theta^{-}) & \text{otherwise} \end{cases}$.

\item Compute the loss between the predicted Q-value and the target Q-value: $L(\theta) = \frac{1}{B}\sum_{j=1}^B(y_j-Q(s_j,a_j;\theta))^2$.
\item Update the Q-network weights using stochastic gradient descent: $\theta \leftarrow \theta - \alpha\nabla_\theta L(\theta)$.
\item Every $C$ steps update the target Q-network weights: $\theta^{-} \leftarrow \tau\theta + (1-\tau)\theta^{-}$.
\item Set $s_t=s_{t+1}$.
\end{enumerate}
\end{enumerate}
\end{enumerate}

In the Deep Q-learning algorithm, the replay memory buffer $D$ is used to store experiences in order to prevent overfitting and stabilize learning. The target Q-network is used to compute the target Q-value in the Q-learning update, and its weights are periodically updated from the Q-network to prevent target overestimation.

The Deep Q-learning algorithm has been successfully applied to various tasks, such as playing Atari games, controlling robots, and playing board games.




The Deep Q-learning algorithm uses experience replay and target networks to improve stability and convergence of the algorithm. Experience replay randomly samples transitions from the replay memory buffer to decorrelate the data and prevent overfitting. Target networks are used to stabilize the training by keeping a separate target network with fixed parameters and periodically updating it with the weights of the online network.

The Deep Q-learning algorithm has been successfully applied to various tasks, such as playing Atari games, controlling robots, and playing board games.








\end{document}

